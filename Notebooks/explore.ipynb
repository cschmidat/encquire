{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Homomorphic Encryption\n",
    "\n",
    "In this notebook, we explore neural network architectures that are compatible with the CKKS encryption scheme, and evaluate their performance. The dataset (which I'm not permitted to upload) consists of ICU patients' EHR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T04:27:17.575778Z",
     "start_time": "2020-07-26T04:27:17.566626Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T04:24:13.446547Z",
     "start_time": "2020-07-26T04:24:11.087721Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../training_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T04:24:19.420482Z",
     "start_time": "2020-07-26T04:24:19.411106Z"
    }
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T04:27:36.677750Z",
     "start_time": "2020-07-26T04:27:36.670216Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(df: pd.DataFrame, colname: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts column colname of df to one-hot encoding.\n",
    "    \"\"\"\n",
    "    return pd.concat([df,pd.get_dummies(df[colname], prefix=colname,dummy_na=True)],axis=1).drop([colname],axis=1)\n",
    "def one_hot_list(df: pd.DataFrame, colnames: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts column in colnames of df to one-hot encoding.\n",
    "    \"\"\"\n",
    "    for colname in colnames:\n",
    "        df = one_hot(df, colname)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T04:27:41.161530Z",
     "start_time": "2020-07-26T04:27:38.674690Z"
    }
   },
   "outputs": [],
   "source": [
    "df = one_hot_list(df, ['ethnicity', 'gender', 'hospital_admit_source',\n",
    "                  'icu_admit_source', 'icu_stay_type',\n",
    "                 'icu_type', 'apache_2_diagnosis', 'apache_3j_diagnosis',\n",
    "                 'apache_3j_bodysystem', 'apache_2_bodysystem'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace nan values by their mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T04:28:03.506118Z",
     "start_time": "2020-07-26T04:28:02.162269Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column] = df[column].fillna(df[column].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:54:13.192906Z",
     "start_time": "2020-06-12T05:54:13.184165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         98.60\n",
       "1        103.42\n",
       "2         77.50\n",
       "3         79.00\n",
       "4         60.70\n",
       "          ...  \n",
       "91208     90.90\n",
       "91209     59.00\n",
       "91210     89.00\n",
       "91211     61.20\n",
       "91212     77.10\n",
       "Name: weight, Length: 99100, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-sample (unbalanced dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:51:43.526088Z",
     "start_time": "2020-06-12T05:51:43.035391Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test = df[-500:]\n",
    "df = df[:-500]\n",
    "df = df.append(df[df['hospital_death']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:51:44.064491Z",
     "start_time": "2020-06-12T05:51:43.530325Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df.set_index(np.random.permutation(df.index))\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:51:44.178264Z",
     "start_time": "2020-06-12T05:51:44.066864Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data = df.drop(columns=['hospital_death'])\n",
    "y_data = df['hospital_death']\n",
    "X_test = df_test.drop(columns=['hospital_death'])\n",
    "y_test = df_test['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:51:44.191206Z",
     "start_time": "2020-06-12T05:51:44.180176Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_data[:-500]\n",
    "y_train = y_data[:-500]\n",
    "\n",
    "X_val = X_data[-500:]\n",
    "y_val = y_data[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:51:44.232684Z",
     "start_time": "2020-06-12T05:51:44.195222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "90731    0\n",
       "90732    0\n",
       "90733    0\n",
       "90734    0\n",
       "90735    0\n",
       "Name: hospital_death, Length: 98600, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:09:29.230869Z",
     "start_time": "2020-06-12T07:09:29.208973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:51:47.144454Z",
     "start_time": "2020-06-12T05:51:44.239074Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    Activation,\n",
    "    AveragePooling2D,\n",
    "    Flatten,\n",
    "    Convolution2D,\n",
    "    MaxPooling2D,\n",
    "    Reshape,\n",
    "    Input,\n",
    "    Dropout\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import (SGD, RMSprop, Adam, Nadam)\n",
    "\n",
    "feature_dim = X_train.shape[1]\n",
    "batch_size = 100\n",
    "\n",
    "def sq_model(input):\n",
    "\n",
    "    def square_activation(x):\n",
    "        return 1/batch_size * x * x\n",
    "\n",
    "    y = Dense(\n",
    "        200 ,\n",
    "        use_bias = True,\n",
    "        input_shape=(feature_dim,),kernel_regularizer='l2',\n",
    "        name=\"fc1\"\n",
    "    )(input)\n",
    "    y = Activation(square_activation)(y)\n",
    "    y = Dropout(rate=0.3)(y, training=True)\n",
    "    y = Dense(\n",
    "        50 ,\n",
    "        use_bias = True,\n",
    "        kernel_regularizer='l2',name=\"fc13\"\n",
    "    )(y)\n",
    "    y = Activation(square_activation)(y)\n",
    "    y = Dropout(rate=0.3)(y, training=True)\n",
    "    y = Dense(2, use_bias=True, name=\"fc_2\", kernel_regularizer='l2')(y)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:51:47.376205Z",
     "start_time": "2020-06-12T05:51:47.147247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/christian/Insight/tf1in/lib64/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 688)]             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 200)               137800    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "fc13 (Dense)                 (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "fc_2 (Dense)                 (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 147,952\n",
      "Trainable params: 147,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x_tf = Input(shape=(feature_dim,), name=\"input\")\n",
    "\n",
    "y_tf = sq_model(x_tf)\n",
    "tf_model = Model(inputs=x_tf, outputs=y_tf)\n",
    "print(tf_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:51:47.866147Z",
     "start_time": "2020-06-12T05:51:47.379213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/christian/Insight/tf1in/lib64/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "def lossf(labels, logits):\n",
    "    return tf.keras.losses.binary_crossentropy(\n",
    "        labels, logits, from_logits=True)\n",
    "\n",
    "#optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "tf_model.compile(\n",
    "    optimizer=optimizer, loss=lossf, metrics=[\"accuracy\",\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:16:14.507884Z",
     "start_time": "2020-06-12T10:41:39.580955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98600 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "98600/98600 [==============================] - 11s 110us/sample - loss: 28382.4957 - acc: 0.7559 - auc: 6.5681e-05 - val_loss: 15546.8562 - val_acc: 0.7920 - val_auc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "98600/98600 [==============================] - 12s 119us/sample - loss: 13402.7234 - acc: 0.7559 - auc: 6.5596e-05 - val_loss: 7926.6233 - val_acc: 0.7700 - val_auc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "98600/98600 [==============================] - 10s 106us/sample - loss: 6341.1842 - acc: 0.7560 - auc: 1.3066e-04 - val_loss: 3649.0457 - val_acc: 0.7480 - val_auc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "98600/98600 [==============================] - 13s 134us/sample - loss: 3221.7156 - acc: 0.7562 - auc: 2.2344e-04 - val_loss: 1848.0925 - val_acc: 0.7700 - val_auc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "98600/98600 [==============================] - 10s 106us/sample - loss: 1714.2716 - acc: 0.7551 - auc: 2.5051e-04 - val_loss: 984.8945 - val_acc: 0.7980 - val_auc: 0.0015\n",
      "Epoch 6/200\n",
      "98600/98600 [==============================] - 10s 100us/sample - loss: 998.2362 - acc: 0.7563 - auc: 4.6598e-04 - val_loss: 709.9584 - val_acc: 0.7620 - val_auc: 0.0056\n",
      "Epoch 7/200\n",
      "98600/98600 [==============================] - 12s 125us/sample - loss: 617.0111 - acc: 0.7572 - auc: 7.0964e-04 - val_loss: 518.6655 - val_acc: 0.7740 - val_auc: 0.0014\n",
      "Epoch 8/200\n",
      "98600/98600 [==============================] - 12s 125us/sample - loss: 392.0114 - acc: 0.7587 - auc: 0.0011 - val_loss: 275.6274 - val_acc: 0.7920 - val_auc: 0.0030\n",
      "Epoch 9/200\n",
      "98600/98600 [==============================] - 12s 123us/sample - loss: 261.9731 - acc: 0.7587 - auc: 0.0019 - val_loss: 174.3205 - val_acc: 0.8020 - val_auc: 0.0074\n",
      "Epoch 10/200\n",
      "98600/98600 [==============================] - 13s 130us/sample - loss: 175.4846 - acc: 0.7667 - auc: 0.0031 - val_loss: 145.8244 - val_acc: 0.7720 - val_auc: 0.0044\n",
      "Epoch 11/200\n",
      "98600/98600 [==============================] - 13s 135us/sample - loss: 112.2768 - acc: 0.7677 - auc: 0.0047 - val_loss: 85.1185 - val_acc: 0.7620 - val_auc: 0.0086\n",
      "Epoch 12/200\n",
      "98600/98600 [==============================] - 14s 138us/sample - loss: 69.5390 - acc: 0.7709 - auc: 0.0064 - val_loss: 51.1275 - val_acc: 0.7940 - val_auc: 0.0030\n",
      "Epoch 13/200\n",
      "98600/98600 [==============================] - 13s 136us/sample - loss: 40.9049 - acc: 0.7758 - auc: 0.0088 - val_loss: 26.5558 - val_acc: 0.8000 - val_auc: 0.0077\n",
      "Epoch 14/200\n",
      "98600/98600 [==============================] - 13s 133us/sample - loss: 23.5117 - acc: 0.7795 - auc: 0.0117 - val_loss: 14.4778 - val_acc: 0.8020 - val_auc: 0.0099\n",
      "Epoch 15/200\n",
      "98600/98600 [==============================] - 14s 141us/sample - loss: 13.3946 - acc: 0.7850 - auc: 0.0154 - val_loss: 10.8360 - val_acc: 0.8040 - val_auc: 0.0124\n",
      "Epoch 16/200\n",
      "98600/98600 [==============================] - 13s 135us/sample - loss: 7.5386 - acc: 0.7893 - auc: 0.0206 - val_loss: 5.4139 - val_acc: 0.8080 - val_auc: 0.0292\n",
      "Epoch 17/200\n",
      "98600/98600 [==============================] - 13s 135us/sample - loss: 4.5761 - acc: 0.7981 - auc: 0.0262 - val_loss: 3.7868 - val_acc: 0.8240 - val_auc: 0.0232\n",
      "Epoch 18/200\n",
      "98600/98600 [==============================] - 13s 133us/sample - loss: 3.1472 - acc: 0.8042 - auc: 0.0327 - val_loss: 2.5857 - val_acc: 0.8300 - val_auc: 0.0340\n",
      "Epoch 19/200\n",
      "98600/98600 [==============================] - 14s 140us/sample - loss: 2.3728 - acc: 0.8126 - auc: 0.0372 - val_loss: 2.0920 - val_acc: 0.8440 - val_auc: 0.0407\n",
      "Epoch 20/200\n",
      "98600/98600 [==============================] - 14s 141us/sample - loss: 1.9640 - acc: 0.8242 - auc: 0.0390 - val_loss: 1.7699 - val_acc: 0.8480 - val_auc: 0.0304\n",
      "Epoch 21/200\n",
      "98600/98600 [==============================] - 13s 136us/sample - loss: 1.7633 - acc: 0.8297 - auc: 0.0385 - val_loss: 1.6695 - val_acc: 0.8480 - val_auc: 0.0355\n",
      "Epoch 22/200\n",
      "98600/98600 [==============================] - 14s 139us/sample - loss: 1.6495 - acc: 0.8340 - auc: 0.0366 - val_loss: 1.5737 - val_acc: 0.8540 - val_auc: 0.0423\n",
      "Epoch 23/200\n",
      "98600/98600 [==============================] - 14s 138us/sample - loss: 1.5747 - acc: 0.8338 - auc: 0.0367 - val_loss: 1.4962 - val_acc: 0.8480 - val_auc: 0.0279\n",
      "Epoch 24/200\n",
      "98600/98600 [==============================] - 11s 111us/sample - loss: 7.1161 - acc: 0.8223 - auc: 0.0345 - val_loss: 4.3345 - val_acc: 0.7360 - val_auc: 0.0270\n",
      "Epoch 25/200\n",
      "98600/98600 [==============================] - 11s 109us/sample - loss: 1.4621 - acc: 0.8379 - auc: 0.0315 - val_loss: 1.3898 - val_acc: 0.8640 - val_auc: 0.0283\n",
      "Epoch 26/200\n",
      "98600/98600 [==============================] - 12s 124us/sample - loss: 1.3773 - acc: 0.8437 - auc: 0.0346 - val_loss: 1.3321 - val_acc: 0.8680 - val_auc: 0.0248\n",
      "Epoch 27/200\n",
      "98600/98600 [==============================] - 14s 146us/sample - loss: 1.3558 - acc: 0.8441 - auc: 0.0362 - val_loss: 1.3280 - val_acc: 0.8660 - val_auc: 0.0355\n",
      "Epoch 28/200\n",
      "98600/98600 [==============================] - 14s 142us/sample - loss: 1.3318 - acc: 0.8458 - auc: 0.0374 - val_loss: 1.3084 - val_acc: 0.8620 - val_auc: 0.0455\n",
      "Epoch 29/200\n",
      "98600/98600 [==============================] - 11s 116us/sample - loss: 1.2979 - acc: 0.8456 - auc: 0.0378 - val_loss: 1.2469 - val_acc: 0.8660 - val_auc: 0.0380\n",
      "Epoch 30/200\n",
      "98600/98600 [==============================] - 11s 110us/sample - loss: 1.2522 - acc: 0.8459 - auc: 0.0379 - val_loss: 1.1875 - val_acc: 0.8640 - val_auc: 0.0406\n",
      "Epoch 31/200\n",
      "98600/98600 [==============================] - 11s 111us/sample - loss: 1.1914 - acc: 0.8453 - auc: 0.0391 - val_loss: 1.1418 - val_acc: 0.8560 - val_auc: 0.0445\n",
      "Epoch 32/200\n",
      "98600/98600 [==============================] - 12s 120us/sample - loss: 1.1296 - acc: 0.8420 - auc: 0.0388 - val_loss: 1.0457 - val_acc: 0.8600 - val_auc: 0.0478\n",
      "Epoch 33/200\n",
      "98600/98600 [==============================] - 15s 153us/sample - loss: 7.1030 - acc: 0.8322 - auc: 0.0359 - val_loss: 0.9674 - val_acc: 0.8500 - val_auc: 0.0380\n",
      "Epoch 34/200\n",
      "98600/98600 [==============================] - 13s 137us/sample - loss: 0.9602 - acc: 0.8485 - auc: 0.0395 - val_loss: 0.9255 - val_acc: 0.8720 - val_auc: 0.0382\n",
      "Epoch 35/200\n",
      "98600/98600 [==============================] - 15s 155us/sample - loss: 0.9390 - acc: 0.8510 - auc: 0.0422 - val_loss: 0.9145 - val_acc: 0.8660 - val_auc: 0.0383\n",
      "Epoch 36/200\n",
      "98600/98600 [==============================] - 12s 126us/sample - loss: 0.9258 - acc: 0.8521 - auc: 0.0426 - val_loss: 0.9028 - val_acc: 0.8640 - val_auc: 0.0491\n",
      "Epoch 37/200\n",
      "98600/98600 [==============================] - 14s 137us/sample - loss: 0.9116 - acc: 0.8522 - auc: 0.0436 - val_loss: 0.8706 - val_acc: 0.8700 - val_auc: 0.0318\n",
      "Epoch 38/200\n",
      "98600/98600 [==============================] - 14s 147us/sample - loss: 0.8918 - acc: 0.8513 - auc: 0.0440 - val_loss: 0.8678 - val_acc: 0.8580 - val_auc: 0.0428\n",
      "Epoch 39/200\n",
      "98600/98600 [==============================] - 15s 150us/sample - loss: 0.8651 - acc: 0.8503 - auc: 0.0428 - val_loss: 0.8393 - val_acc: 0.8620 - val_auc: 0.0220\n",
      "Epoch 40/200\n",
      "98600/98600 [==============================] - 15s 149us/sample - loss: 0.8287 - acc: 0.8500 - auc: 0.0423 - val_loss: 0.7821 - val_acc: 0.8620 - val_auc: 0.0532\n",
      "Epoch 41/200\n",
      "98600/98600 [==============================] - 16s 164us/sample - loss: 0.7853 - acc: 0.8481 - auc: 0.0414 - val_loss: 0.7451 - val_acc: 0.8660 - val_auc: 0.0303\n",
      "Epoch 42/200\n",
      "98600/98600 [==============================] - 16s 163us/sample - loss: 6.6613 - acc: 0.8345 - auc: 0.0363 - val_loss: 0.6979 - val_acc: 0.8700 - val_auc: 0.0292\n",
      "Epoch 43/200\n",
      "98600/98600 [==============================] - 18s 180us/sample - loss: 0.7033 - acc: 0.8503 - auc: 0.0409 - val_loss: 0.6745 - val_acc: 0.8700 - val_auc: 0.0445\n",
      "Epoch 44/200\n",
      "98600/98600 [==============================] - 20s 207us/sample - loss: 0.6863 - acc: 0.8534 - auc: 0.0432 - val_loss: 0.6780 - val_acc: 0.8660 - val_auc: 0.0524\n",
      "Epoch 45/200\n",
      "98600/98600 [==============================] - 13s 131us/sample - loss: 0.6801 - acc: 0.8528 - auc: 0.0441 - val_loss: 0.6449 - val_acc: 0.8760 - val_auc: 0.0386\n",
      "Epoch 46/200\n",
      "98600/98600 [==============================] - 12s 125us/sample - loss: 0.6706 - acc: 0.8536 - auc: 0.0446 - val_loss: 0.6527 - val_acc: 0.8560 - val_auc: 0.0529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200\n",
      "98600/98600 [==============================] - 15s 153us/sample - loss: 0.6592 - acc: 0.8527 - auc: 0.0457 - val_loss: 0.6371 - val_acc: 0.8740 - val_auc: 0.0433\n",
      "Epoch 48/200\n",
      "98600/98600 [==============================] - 13s 132us/sample - loss: 0.6447 - acc: 0.8534 - auc: 0.0450 - val_loss: 0.6306 - val_acc: 0.8660 - val_auc: 0.0301\n",
      "Epoch 49/200\n",
      "98600/98600 [==============================] - 13s 127us/sample - loss: 0.6271 - acc: 0.8520 - auc: 0.0438 - val_loss: 0.5907 - val_acc: 0.8680 - val_auc: 0.0237\n",
      "Epoch 50/200\n",
      "98600/98600 [==============================] - 12s 123us/sample - loss: 0.6063 - acc: 0.8506 - auc: 0.0432 - val_loss: 0.5908 - val_acc: 0.8640 - val_auc: 0.0418\n",
      "Epoch 51/200\n",
      "98600/98600 [==============================] - 12s 125us/sample - loss: 0.5806 - acc: 0.8479 - auc: 0.0438 - val_loss: 0.6476 - val_acc: 0.8220 - val_auc: 0.0572\n",
      "Epoch 52/200\n",
      "98600/98600 [==============================] - 12s 126us/sample - loss: 2.5719 - acc: 0.8418 - auc: 0.0401 - val_loss: 0.5040 - val_acc: 0.8780 - val_auc: 0.0451\n",
      "Epoch 53/200\n",
      "98600/98600 [==============================] - 12s 122us/sample - loss: 0.5341 - acc: 0.8537 - auc: 0.0452 - val_loss: 0.5223 - val_acc: 0.8680 - val_auc: 0.0578\n",
      "Epoch 54/200\n",
      "98600/98600 [==============================] - 12s 119us/sample - loss: 0.5276 - acc: 0.8552 - auc: 0.0466 - val_loss: 0.5137 - val_acc: 0.8680 - val_auc: 0.0465\n",
      "Epoch 55/200\n",
      "98600/98600 [==============================] - 15s 157us/sample - loss: 0.5224 - acc: 0.8547 - auc: 0.0463 - val_loss: 0.5087 - val_acc: 0.8620 - val_auc: 0.0442\n",
      "Epoch 56/200\n",
      "98600/98600 [==============================] - 11s 110us/sample - loss: 0.5164 - acc: 0.8536 - auc: 0.0466 - val_loss: 0.4941 - val_acc: 0.8700 - val_auc: 0.0475\n",
      "Epoch 57/200\n",
      "98600/98600 [==============================] - 8s 83us/sample - loss: 0.5095 - acc: 0.8515 - auc: 0.0471 - val_loss: 0.5102 - val_acc: 0.8580 - val_auc: 0.0492\n",
      "Epoch 58/200\n",
      "98600/98600 [==============================] - 8s 85us/sample - loss: 0.5001 - acc: 0.8508 - auc: 0.0455 - val_loss: 0.4879 - val_acc: 0.8620 - val_auc: 0.0413\n",
      "Epoch 59/200\n",
      "98600/98600 [==============================] - 8s 80us/sample - loss: 0.4913 - acc: 0.8496 - auc: 0.0448 - val_loss: 0.4698 - val_acc: 0.8680 - val_auc: 0.0468\n",
      "Epoch 60/200\n",
      "98600/98600 [==============================] - 8s 85us/sample - loss: 1.0634 - acc: 0.8387 - auc: 0.0412 - val_loss: 0.4309 - val_acc: 0.8680 - val_auc: 0.0419\n",
      "Epoch 61/200\n",
      "98600/98600 [==============================] - 8s 80us/sample - loss: 0.4512 - acc: 0.8522 - auc: 0.0453 - val_loss: 0.4236 - val_acc: 0.8720 - val_auc: 0.0452\n",
      "Epoch 62/200\n",
      "98600/98600 [==============================] - 9s 91us/sample - loss: 0.4468 - acc: 0.8528 - auc: 0.0461 - val_loss: 0.4335 - val_acc: 0.8660 - val_auc: 0.0486\n",
      "Epoch 63/200\n",
      "98600/98600 [==============================] - 8s 86us/sample - loss: 0.4446 - acc: 0.8526 - auc: 0.0464 - val_loss: 0.4486 - val_acc: 0.8560 - val_auc: 0.0672\n",
      "Epoch 64/200\n",
      "98600/98600 [==============================] - 9s 87us/sample - loss: 0.4435 - acc: 0.8511 - auc: 0.0477 - val_loss: 0.4217 - val_acc: 0.8740 - val_auc: 0.0395\n",
      "Epoch 65/200\n",
      "98600/98600 [==============================] - 7s 74us/sample - loss: 0.4398 - acc: 0.8506 - auc: 0.0467 - val_loss: 0.4528 - val_acc: 0.8680 - val_auc: 0.0337\n",
      "Epoch 66/200\n",
      "98600/98600 [==============================] - 6s 66us/sample - loss: 0.4333 - acc: 0.8511 - auc: 0.0446 - val_loss: 0.4260 - val_acc: 0.8780 - val_auc: 0.0508\n",
      "Epoch 67/200\n",
      "98600/98600 [==============================] - 10s 101us/sample - loss: 0.4305 - acc: 0.8492 - auc: 0.0455 - val_loss: 0.4163 - val_acc: 0.8580 - val_auc: 0.0345\n",
      "Epoch 68/200\n",
      "98600/98600 [==============================] - 16s 165us/sample - loss: 0.4252 - acc: 0.8494 - auc: 0.0444 - val_loss: 0.4151 - val_acc: 0.8660 - val_auc: 0.0542\n",
      "Epoch 69/200\n",
      "98600/98600 [==============================] - 14s 145us/sample - loss: 0.4115 - acc: 0.8496 - auc: 0.0457 - val_loss: 0.3928 - val_acc: 0.8680 - val_auc: 0.0485\n",
      "Epoch 70/200\n",
      "98600/98600 [==============================] - 14s 143us/sample - loss: 0.4093 - acc: 0.8486 - auc: 0.0459 - val_loss: 0.3982 - val_acc: 0.8580 - val_auc: 0.0445\n",
      "Epoch 71/200\n",
      "98600/98600 [==============================] - 13s 134us/sample - loss: 0.4019 - acc: 0.8489 - auc: 0.0452 - val_loss: 0.3909 - val_acc: 0.8680 - val_auc: 0.0484\n",
      "Epoch 72/200\n",
      "98600/98600 [==============================] - 13s 131us/sample - loss: 0.3997 - acc: 0.8494 - auc: 0.0441 - val_loss: 0.3822 - val_acc: 0.8660 - val_auc: 0.0475\n",
      "Epoch 73/200\n",
      "98600/98600 [==============================] - 12s 126us/sample - loss: 0.3962 - acc: 0.8489 - auc: 0.0451 - val_loss: 0.3682 - val_acc: 0.8700 - val_auc: 0.0416\n",
      "Epoch 74/200\n",
      "98600/98600 [==============================] - 13s 129us/sample - loss: 0.3925 - acc: 0.8491 - auc: 0.0462 - val_loss: 0.3576 - val_acc: 0.8660 - val_auc: 0.0505\n",
      "Epoch 75/200\n",
      "98600/98600 [==============================] - 13s 129us/sample - loss: 0.3923 - acc: 0.8494 - auc: 0.0460 - val_loss: 0.3859 - val_acc: 0.8580 - val_auc: 0.0470\n",
      "Epoch 76/200\n",
      "98600/98600 [==============================] - 14s 141us/sample - loss: 0.3922 - acc: 0.8499 - auc: 0.0450 - val_loss: 0.3722 - val_acc: 0.8680 - val_auc: 0.0531\n",
      "Epoch 77/200\n",
      "98600/98600 [==============================] - 15s 157us/sample - loss: 0.3954 - acc: 0.8500 - auc: 0.0451 - val_loss: 0.3752 - val_acc: 0.8640 - val_auc: 0.0495\n",
      "Epoch 78/200\n",
      "98600/98600 [==============================] - 15s 156us/sample - loss: 0.3905 - acc: 0.8499 - auc: 0.0443 - val_loss: 0.4048 - val_acc: 0.8520 - val_auc: 0.0597\n",
      "Epoch 79/200\n",
      "98600/98600 [==============================] - 15s 148us/sample - loss: 0.3898 - acc: 0.8497 - auc: 0.0443 - val_loss: 0.3797 - val_acc: 0.8680 - val_auc: 0.0568\n",
      "Epoch 80/200\n",
      "98600/98600 [==============================] - 15s 152us/sample - loss: 0.3899 - acc: 0.8496 - auc: 0.0455 - val_loss: 0.3848 - val_acc: 0.8600 - val_auc: 0.0435\n",
      "Epoch 81/200\n",
      "98600/98600 [==============================] - 14s 145us/sample - loss: 0.3908 - acc: 0.8499 - auc: 0.0432 - val_loss: 0.3730 - val_acc: 0.8600 - val_auc: 0.0402\n",
      "Epoch 82/200\n",
      "98600/98600 [==============================] - 15s 155us/sample - loss: 0.3883 - acc: 0.8505 - auc: 0.0444 - val_loss: 0.3889 - val_acc: 0.8600 - val_auc: 0.0154\n",
      "Epoch 83/200\n",
      "98600/98600 [==============================] - 16s 166us/sample - loss: 0.3888 - acc: 0.8494 - auc: 0.0447 - val_loss: 0.3689 - val_acc: 0.8620 - val_auc: 0.0401\n",
      "Epoch 84/200\n",
      "98600/98600 [==============================] - 15s 148us/sample - loss: 0.3888 - acc: 0.8505 - auc: 0.0442 - val_loss: 0.3888 - val_acc: 0.8600 - val_auc: 0.0305\n",
      "Epoch 85/200\n",
      "98600/98600 [==============================] - 13s 132us/sample - loss: 0.3901 - acc: 0.8491 - auc: 0.0442 - val_loss: 0.3656 - val_acc: 0.8760 - val_auc: 0.0428\n",
      "Epoch 86/200\n",
      "98600/98600 [==============================] - 13s 131us/sample - loss: 0.3886 - acc: 0.8500 - auc: 0.0449 - val_loss: 0.3925 - val_acc: 0.8700 - val_auc: 0.0417\n",
      "Epoch 87/200\n",
      "98600/98600 [==============================] - 13s 130us/sample - loss: 0.3876 - acc: 0.8505 - auc: 0.0459 - val_loss: 0.3900 - val_acc: 0.8620 - val_auc: 0.0495\n",
      "Epoch 88/200\n",
      "98600/98600 [==============================] - 12s 117us/sample - loss: 0.3898 - acc: 0.8495 - auc: 0.0446 - val_loss: 0.3570 - val_acc: 0.8640 - val_auc: 0.0422\n",
      "Epoch 89/200\n",
      "98600/98600 [==============================] - 13s 133us/sample - loss: 0.3898 - acc: 0.8510 - auc: 0.0436 - val_loss: 0.3768 - val_acc: 0.8600 - val_auc: 0.0321\n",
      "Epoch 90/200\n",
      "98600/98600 [==============================] - 12s 119us/sample - loss: 0.3882 - acc: 0.8503 - auc: 0.0456 - val_loss: 0.3701 - val_acc: 0.8620 - val_auc: 0.0388\n",
      "Epoch 91/200\n",
      "98600/98600 [==============================] - 11s 116us/sample - loss: 0.3865 - acc: 0.8504 - auc: 0.0453 - val_loss: 0.3820 - val_acc: 0.8600 - val_auc: 0.0270\n",
      "Epoch 92/200\n",
      "98600/98600 [==============================] - 11s 113us/sample - loss: 0.3895 - acc: 0.8497 - auc: 0.0457 - val_loss: 0.3768 - val_acc: 0.8580 - val_auc: 0.0465\n",
      "Epoch 93/200\n",
      "98600/98600 [==============================] - 10s 104us/sample - loss: 0.3871 - acc: 0.8508 - auc: 0.0449 - val_loss: 0.3646 - val_acc: 0.8640 - val_auc: 0.0422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "98600/98600 [==============================] - 13s 127us/sample - loss: 0.3887 - acc: 0.8504 - auc: 0.0432 - val_loss: 0.3768 - val_acc: 0.8680 - val_auc: 0.0462\n",
      "Epoch 95/200\n",
      "98600/98600 [==============================] - 10s 106us/sample - loss: 0.3891 - acc: 0.8501 - auc: 0.0457 - val_loss: 0.3908 - val_acc: 0.8580 - val_auc: 0.0321\n",
      "Epoch 96/200\n",
      "98600/98600 [==============================] - 9s 93us/sample - loss: 0.3892 - acc: 0.8501 - auc: 0.0460 - val_loss: 0.3876 - val_acc: 0.8580 - val_auc: 0.0441\n",
      "Epoch 97/200\n",
      "98600/98600 [==============================] - 10s 99us/sample - loss: 0.3886 - acc: 0.8503 - auc: 0.0461 - val_loss: 0.3720 - val_acc: 0.8620 - val_auc: 0.0482\n",
      "Epoch 98/200\n",
      "98600/98600 [==============================] - 8s 83us/sample - loss: 0.3887 - acc: 0.8503 - auc: 0.0452 - val_loss: 0.3611 - val_acc: 0.8760 - val_auc: 0.0486\n",
      "Epoch 99/200\n",
      "98600/98600 [==============================] - 9s 87us/sample - loss: 0.3892 - acc: 0.8497 - auc: 0.0458 - val_loss: 0.3605 - val_acc: 0.8620 - val_auc: 0.0617\n",
      "Epoch 100/200\n",
      "98600/98600 [==============================] - 7s 66us/sample - loss: 0.3876 - acc: 0.8506 - auc: 0.0452 - val_loss: 0.3701 - val_acc: 0.8660 - val_auc: 0.0351\n",
      "Epoch 101/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3883 - acc: 0.8500 - auc: 0.0463 - val_loss: 0.3652 - val_acc: 0.8720 - val_auc: 0.0283\n",
      "Epoch 102/200\n",
      "98600/98600 [==============================] - 8s 77us/sample - loss: 0.3900 - acc: 0.8492 - auc: 0.0452 - val_loss: 0.3651 - val_acc: 0.8540 - val_auc: 0.0552\n",
      "Epoch 103/200\n",
      "98600/98600 [==============================] - 8s 81us/sample - loss: 0.3900 - acc: 0.8498 - auc: 0.0453 - val_loss: 0.3588 - val_acc: 0.8680 - val_auc: 0.0484\n",
      "Epoch 104/200\n",
      "98600/98600 [==============================] - 6s 64us/sample - loss: 0.3903 - acc: 0.8500 - auc: 0.0458 - val_loss: 0.3686 - val_acc: 0.8680 - val_auc: 0.0447\n",
      "Epoch 105/200\n",
      "98600/98600 [==============================] - 7s 71us/sample - loss: 0.3895 - acc: 0.8501 - auc: 0.0454 - val_loss: 0.3914 - val_acc: 0.8740 - val_auc: 0.0500\n",
      "Epoch 106/200\n",
      "98600/98600 [==============================] - 7s 67us/sample - loss: 0.3893 - acc: 0.8501 - auc: 0.0455 - val_loss: 0.3878 - val_acc: 0.8500 - val_auc: 0.0515\n",
      "Epoch 107/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3870 - acc: 0.8509 - auc: 0.0464 - val_loss: 0.3919 - val_acc: 0.8500 - val_auc: 0.0560\n",
      "Epoch 108/200\n",
      "98600/98600 [==============================] - 8s 76us/sample - loss: 0.3866 - acc: 0.8507 - auc: 0.0462 - val_loss: 0.3477 - val_acc: 0.8780 - val_auc: 0.0382\n",
      "Epoch 109/200\n",
      "98600/98600 [==============================] - 8s 80us/sample - loss: 0.3882 - acc: 0.8505 - auc: 0.0454 - val_loss: 0.3661 - val_acc: 0.8660 - val_auc: 0.0487\n",
      "Epoch 110/200\n",
      "98600/98600 [==============================] - 8s 78us/sample - loss: 0.3899 - acc: 0.8498 - auc: 0.0454 - val_loss: 0.3989 - val_acc: 0.8580 - val_auc: 0.0329\n",
      "Epoch 111/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3895 - acc: 0.8498 - auc: 0.0451 - val_loss: 0.4018 - val_acc: 0.8640 - val_auc: 0.0354\n",
      "Epoch 112/200\n",
      "98600/98600 [==============================] - 8s 80us/sample - loss: 0.3891 - acc: 0.8509 - auc: 0.0456 - val_loss: 0.3951 - val_acc: 0.8640 - val_auc: 0.0482\n",
      "Epoch 113/200\n",
      "98600/98600 [==============================] - 7s 74us/sample - loss: 0.3886 - acc: 0.8494 - auc: 0.0458 - val_loss: 0.3771 - val_acc: 0.8580 - val_auc: 0.0320\n",
      "Epoch 114/200\n",
      "98600/98600 [==============================] - 6s 64us/sample - loss: 0.3884 - acc: 0.8504 - auc: 0.0467 - val_loss: 0.3638 - val_acc: 0.8660 - val_auc: 0.0502\n",
      "Epoch 115/200\n",
      "98600/98600 [==============================] - 7s 67us/sample - loss: 0.3885 - acc: 0.8498 - auc: 0.0457 - val_loss: 0.3722 - val_acc: 0.8800 - val_auc: 0.0363\n",
      "Epoch 116/200\n",
      "98600/98600 [==============================] - 7s 71us/sample - loss: 0.3886 - acc: 0.8504 - auc: 0.0455 - val_loss: 0.3739 - val_acc: 0.8640 - val_auc: 0.0451\n",
      "Epoch 117/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3884 - acc: 0.8497 - auc: 0.0464 - val_loss: 0.3744 - val_acc: 0.8700 - val_auc: 0.0414\n",
      "Epoch 118/200\n",
      "98600/98600 [==============================] - 8s 77us/sample - loss: 0.3897 - acc: 0.8489 - auc: 0.0467 - val_loss: 0.3814 - val_acc: 0.8580 - val_auc: 0.0477\n",
      "Epoch 119/200\n",
      "98600/98600 [==============================] - 7s 68us/sample - loss: 0.3878 - acc: 0.8508 - auc: 0.0454 - val_loss: 0.3878 - val_acc: 0.8600 - val_auc: 0.0353\n",
      "Epoch 120/200\n",
      "98600/98600 [==============================] - 9s 93us/sample - loss: 0.3883 - acc: 0.8491 - auc: 0.0464 - val_loss: 0.3730 - val_acc: 0.8700 - val_auc: 0.0397\n",
      "Epoch 121/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3888 - acc: 0.8499 - auc: 0.0480 - val_loss: 0.3606 - val_acc: 0.8720 - val_auc: 0.0446\n",
      "Epoch 122/200\n",
      "98600/98600 [==============================] - 7s 71us/sample - loss: 0.3879 - acc: 0.8498 - auc: 0.0466 - val_loss: 0.3739 - val_acc: 0.8620 - val_auc: 0.0390\n",
      "Epoch 123/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3870 - acc: 0.8506 - auc: 0.0458 - val_loss: 0.3593 - val_acc: 0.8680 - val_auc: 0.0321\n",
      "Epoch 124/200\n",
      "98600/98600 [==============================] - 8s 81us/sample - loss: 0.3878 - acc: 0.8508 - auc: 0.0460 - val_loss: 0.3705 - val_acc: 0.8600 - val_auc: 0.0560\n",
      "Epoch 125/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3878 - acc: 0.8502 - auc: 0.0475 - val_loss: 0.3684 - val_acc: 0.8620 - val_auc: 0.0403\n",
      "Epoch 126/200\n",
      "98600/98600 [==============================] - 8s 81us/sample - loss: 0.3882 - acc: 0.8503 - auc: 0.0456 - val_loss: 0.3788 - val_acc: 0.8640 - val_auc: 0.0286\n",
      "Epoch 127/200\n",
      "98600/98600 [==============================] - 8s 80us/sample - loss: 0.3893 - acc: 0.8496 - auc: 0.0461 - val_loss: 0.3786 - val_acc: 0.8680 - val_auc: 0.0513\n",
      "Epoch 128/200\n",
      "98600/98600 [==============================] - 8s 80us/sample - loss: 0.3877 - acc: 0.8507 - auc: 0.0466 - val_loss: 0.3758 - val_acc: 0.8660 - val_auc: 0.0551\n",
      "Epoch 129/200\n",
      "98600/98600 [==============================] - 13s 135us/sample - loss: 0.3873 - acc: 0.8507 - auc: 0.0464 - val_loss: 0.3602 - val_acc: 0.8580 - val_auc: 0.0533\n",
      "Epoch 130/200\n",
      "98600/98600 [==============================] - 13s 132us/sample - loss: 0.3880 - acc: 0.8499 - auc: 0.0458 - val_loss: 0.3685 - val_acc: 0.8680 - val_auc: 0.0513\n",
      "Epoch 131/200\n",
      "98600/98600 [==============================] - 15s 148us/sample - loss: 0.3883 - acc: 0.8494 - auc: 0.0472 - val_loss: 0.3780 - val_acc: 0.8620 - val_auc: 0.0635\n",
      "Epoch 132/200\n",
      "98600/98600 [==============================] - 17s 170us/sample - loss: 0.3898 - acc: 0.8490 - auc: 0.0467 - val_loss: 0.3727 - val_acc: 0.8520 - val_auc: 0.0575\n",
      "Epoch 133/200\n",
      "98600/98600 [==============================] - 11s 116us/sample - loss: 0.3888 - acc: 0.8504 - auc: 0.0450 - val_loss: 0.3670 - val_acc: 0.8700 - val_auc: 0.0386\n",
      "Epoch 134/200\n",
      "98600/98600 [==============================] - 10s 105us/sample - loss: 0.3860 - acc: 0.8513 - auc: 0.0463 - val_loss: 0.3579 - val_acc: 0.8700 - val_auc: 0.0400\n",
      "Epoch 135/200\n",
      "98600/98600 [==============================] - 14s 140us/sample - loss: 0.3885 - acc: 0.8506 - auc: 0.0456 - val_loss: 0.3651 - val_acc: 0.8740 - val_auc: 0.0484\n",
      "Epoch 136/200\n",
      "98600/98600 [==============================] - 15s 148us/sample - loss: 0.3876 - acc: 0.8507 - auc: 0.0464 - val_loss: 0.3742 - val_acc: 0.8720 - val_auc: 0.0458\n",
      "Epoch 137/200\n",
      "98600/98600 [==============================] - 14s 139us/sample - loss: 0.3874 - acc: 0.8500 - auc: 0.0469 - val_loss: 0.3714 - val_acc: 0.8600 - val_auc: 0.0434\n",
      "Epoch 138/200\n",
      "98600/98600 [==============================] - 14s 144us/sample - loss: 0.3869 - acc: 0.8513 - auc: 0.0460 - val_loss: 0.3613 - val_acc: 0.8600 - val_auc: 0.0406\n",
      "Epoch 139/200\n",
      "98600/98600 [==============================] - 12s 123us/sample - loss: 0.3871 - acc: 0.8515 - auc: 0.0458 - val_loss: 0.3789 - val_acc: 0.8620 - val_auc: 0.0416\n",
      "Epoch 140/200\n",
      "98600/98600 [==============================] - 14s 139us/sample - loss: 0.3875 - acc: 0.8507 - auc: 0.0468 - val_loss: 0.3754 - val_acc: 0.8600 - val_auc: 0.0321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200\n",
      "98600/98600 [==============================] - 12s 123us/sample - loss: 0.3859 - acc: 0.8517 - auc: 0.0453 - val_loss: 0.3622 - val_acc: 0.8700 - val_auc: 0.0433\n",
      "Epoch 142/200\n",
      "98600/98600 [==============================] - 12s 125us/sample - loss: 0.3841 - acc: 0.8525 - auc: 0.0451 - val_loss: 0.3970 - val_acc: 0.8660 - val_auc: 0.0398\n",
      "Epoch 143/200\n",
      "98600/98600 [==============================] - 12s 126us/sample - loss: 0.3857 - acc: 0.8525 - auc: 0.0449 - val_loss: 0.3564 - val_acc: 0.8760 - val_auc: 0.0402\n",
      "Epoch 144/200\n",
      "98600/98600 [==============================] - 12s 126us/sample - loss: 0.3860 - acc: 0.8518 - auc: 0.0462 - val_loss: 0.3978 - val_acc: 0.8580 - val_auc: 0.0385\n",
      "Epoch 145/200\n",
      "98600/98600 [==============================] - 12s 123us/sample - loss: 0.3864 - acc: 0.8520 - auc: 0.0447 - val_loss: 0.3664 - val_acc: 0.8700 - val_auc: 0.0318\n",
      "Epoch 146/200\n",
      "98600/98600 [==============================] - 12s 122us/sample - loss: 0.3850 - acc: 0.8530 - auc: 0.0446 - val_loss: 0.3615 - val_acc: 0.8720 - val_auc: 0.0455\n",
      "Epoch 147/200\n",
      "98600/98600 [==============================] - 12s 124us/sample - loss: 0.3846 - acc: 0.8526 - auc: 0.0454 - val_loss: 0.3671 - val_acc: 0.8660 - val_auc: 0.0540\n",
      "Epoch 148/200\n",
      "98600/98600 [==============================] - 12s 124us/sample - loss: 0.3850 - acc: 0.8532 - auc: 0.0443 - val_loss: 0.3599 - val_acc: 0.8700 - val_auc: 0.0472\n",
      "Epoch 149/200\n",
      "98600/98600 [==============================] - 11s 110us/sample - loss: 0.3827 - acc: 0.8539 - auc: 0.0442 - val_loss: 0.3608 - val_acc: 0.8780 - val_auc: 0.0351\n",
      "Epoch 150/200\n",
      "98600/98600 [==============================] - 12s 119us/sample - loss: 0.3850 - acc: 0.8535 - auc: 0.0437 - val_loss: 0.3892 - val_acc: 0.8580 - val_auc: 0.0596\n",
      "Epoch 151/200\n",
      "98600/98600 [==============================] - 7s 69us/sample - loss: 0.3838 - acc: 0.8548 - auc: 0.0427 - val_loss: 0.3706 - val_acc: 0.8660 - val_auc: 0.0575\n",
      "Epoch 152/200\n",
      "98600/98600 [==============================] - 6s 63us/sample - loss: 0.3839 - acc: 0.8540 - auc: 0.0428 - val_loss: 0.3810 - val_acc: 0.8600 - val_auc: 0.0317\n",
      "Epoch 153/200\n",
      "98600/98600 [==============================] - 6s 59us/sample - loss: 0.3837 - acc: 0.8525 - auc: 0.0445 - val_loss: 0.3744 - val_acc: 0.8580 - val_auc: 0.0561\n",
      "Epoch 154/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3848 - acc: 0.8536 - auc: 0.0442 - val_loss: 0.3585 - val_acc: 0.8700 - val_auc: 0.0403\n",
      "Epoch 155/200\n",
      "98600/98600 [==============================] - 8s 77us/sample - loss: 0.3830 - acc: 0.8540 - auc: 0.0449 - val_loss: 0.3620 - val_acc: 0.8700 - val_auc: 0.0385\n",
      "Epoch 156/200\n",
      "98600/98600 [==============================] - 8s 83us/sample - loss: 0.3822 - acc: 0.8530 - auc: 0.0447 - val_loss: 0.3692 - val_acc: 0.8680 - val_auc: 0.0553\n",
      "Epoch 157/200\n",
      "98600/98600 [==============================] - 7s 66us/sample - loss: 0.3848 - acc: 0.8529 - auc: 0.0436 - val_loss: 0.3851 - val_acc: 0.8600 - val_auc: 0.0221\n",
      "Epoch 158/200\n",
      "98600/98600 [==============================] - 7s 71us/sample - loss: 0.3829 - acc: 0.8534 - auc: 0.0449 - val_loss: 0.3834 - val_acc: 0.8680 - val_auc: 0.0352\n",
      "Epoch 159/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3835 - acc: 0.8543 - auc: 0.0435 - val_loss: 0.3732 - val_acc: 0.8700 - val_auc: 0.0495\n",
      "Epoch 160/200\n",
      "98600/98600 [==============================] - 9s 91us/sample - loss: 0.3834 - acc: 0.8532 - auc: 0.0440 - val_loss: 0.3625 - val_acc: 0.8640 - val_auc: 0.0571\n",
      "Epoch 161/200\n",
      "98600/98600 [==============================] - 7s 69us/sample - loss: 0.3858 - acc: 0.8519 - auc: 0.0419 - val_loss: 0.3958 - val_acc: 0.8640 - val_auc: 0.0255\n",
      "Epoch 162/200\n",
      "98600/98600 [==============================] - 6s 63us/sample - loss: 0.3848 - acc: 0.8529 - auc: 0.0439 - val_loss: 0.3757 - val_acc: 0.8800 - val_auc: 0.0323\n",
      "Epoch 163/200\n",
      "98600/98600 [==============================] - 6s 66us/sample - loss: 0.3831 - acc: 0.8530 - auc: 0.0449 - val_loss: 0.3601 - val_acc: 0.8660 - val_auc: 0.0562\n",
      "Epoch 164/200\n",
      "98600/98600 [==============================] - 6s 63us/sample - loss: 0.3833 - acc: 0.8534 - auc: 0.0437 - val_loss: 0.3846 - val_acc: 0.8760 - val_auc: 0.0403\n",
      "Epoch 165/200\n",
      "98600/98600 [==============================] - 6s 65us/sample - loss: 0.3831 - acc: 0.8542 - auc: 0.0434 - val_loss: 0.3711 - val_acc: 0.8800 - val_auc: 0.0459\n",
      "Epoch 166/200\n",
      "98600/98600 [==============================] - 6s 62us/sample - loss: 0.3851 - acc: 0.8528 - auc: 0.0437 - val_loss: 0.3795 - val_acc: 0.8640 - val_auc: 0.0402\n",
      "Epoch 167/200\n",
      "98600/98600 [==============================] - 7s 70us/sample - loss: 0.3842 - acc: 0.8541 - auc: 0.0437 - val_loss: 0.3584 - val_acc: 0.8680 - val_auc: 0.0433\n",
      "Epoch 168/200\n",
      "98600/98600 [==============================] - 6s 59us/sample - loss: 0.3844 - acc: 0.8523 - auc: 0.0434 - val_loss: 0.3644 - val_acc: 0.8620 - val_auc: 0.0387\n",
      "Epoch 169/200\n",
      "98600/98600 [==============================] - 7s 66us/sample - loss: 0.3841 - acc: 0.8535 - auc: 0.0437 - val_loss: 0.3986 - val_acc: 0.8640 - val_auc: 0.0188\n",
      "Epoch 170/200\n",
      "98600/98600 [==============================] - 7s 67us/sample - loss: 0.3840 - acc: 0.8524 - auc: 0.0437 - val_loss: 0.3799 - val_acc: 0.8780 - val_auc: 0.0525\n",
      "Epoch 171/200\n",
      "98600/98600 [==============================] - 6s 61us/sample - loss: 0.3837 - acc: 0.8537 - auc: 0.0442 - val_loss: 0.3544 - val_acc: 0.8720 - val_auc: 0.0505\n",
      "Epoch 172/200\n",
      "98600/98600 [==============================] - 6s 63us/sample - loss: 0.3839 - acc: 0.8534 - auc: 0.0439 - val_loss: 0.3796 - val_acc: 0.8660 - val_auc: 0.0524\n",
      "Epoch 173/200\n",
      "98600/98600 [==============================] - 7s 66us/sample - loss: 0.3830 - acc: 0.8531 - auc: 0.0438 - val_loss: 0.3661 - val_acc: 0.8700 - val_auc: 0.0536\n",
      "Epoch 174/200\n",
      "98600/98600 [==============================] - 7s 68us/sample - loss: 0.3832 - acc: 0.8539 - auc: 0.0438 - val_loss: 0.3629 - val_acc: 0.8680 - val_auc: 0.0533\n",
      "Epoch 175/200\n",
      "98600/98600 [==============================] - 6s 64us/sample - loss: 0.3831 - acc: 0.8547 - auc: 0.0429 - val_loss: 0.3705 - val_acc: 0.8700 - val_auc: 0.0463\n",
      "Epoch 176/200\n",
      "98600/98600 [==============================] - 7s 67us/sample - loss: 0.3847 - acc: 0.8535 - auc: 0.0433 - val_loss: 0.3779 - val_acc: 0.8660 - val_auc: 0.0483\n",
      "Epoch 177/200\n",
      "98600/98600 [==============================] - 6s 65us/sample - loss: 0.3837 - acc: 0.8538 - auc: 0.0424 - val_loss: 0.3676 - val_acc: 0.8700 - val_auc: 0.0417\n",
      "Epoch 178/200\n",
      "98600/98600 [==============================] - 6s 65us/sample - loss: 0.3843 - acc: 0.8538 - auc: 0.0438 - val_loss: 0.3683 - val_acc: 0.8740 - val_auc: 0.0336\n",
      "Epoch 179/200\n",
      "98600/98600 [==============================] - 7s 68us/sample - loss: 0.3829 - acc: 0.8543 - auc: 0.0444 - val_loss: 0.3638 - val_acc: 0.8660 - val_auc: 0.0466\n",
      "Epoch 180/200\n",
      "98600/98600 [==============================] - 7s 67us/sample - loss: 0.3814 - acc: 0.8546 - auc: 0.0439 - val_loss: 0.3588 - val_acc: 0.8700 - val_auc: 0.0386\n",
      "Epoch 181/200\n",
      "98600/98600 [==============================] - 6s 65us/sample - loss: 0.3832 - acc: 0.8536 - auc: 0.0438 - val_loss: 0.3698 - val_acc: 0.8700 - val_auc: 0.0423\n",
      "Epoch 182/200\n",
      "98600/98600 [==============================] - 7s 69us/sample - loss: 0.3843 - acc: 0.8535 - auc: 0.0418 - val_loss: 0.3742 - val_acc: 0.8680 - val_auc: 0.0302\n",
      "Epoch 183/200\n",
      "98600/98600 [==============================] - 7s 72us/sample - loss: 0.3846 - acc: 0.8537 - auc: 0.0429 - val_loss: 0.3690 - val_acc: 0.8760 - val_auc: 0.0486\n",
      "Epoch 184/200\n",
      "98600/98600 [==============================] - 7s 72us/sample - loss: 0.3833 - acc: 0.8540 - auc: 0.0428 - val_loss: 0.3579 - val_acc: 0.8640 - val_auc: 0.0473\n",
      "Epoch 185/200\n",
      "98600/98600 [==============================] - 8s 81us/sample - loss: 0.3833 - acc: 0.8532 - auc: 0.0439 - val_loss: 0.3711 - val_acc: 0.8780 - val_auc: 0.0399\n",
      "Epoch 186/200\n",
      "98600/98600 [==============================] - 7s 71us/sample - loss: 0.3840 - acc: 0.8532 - auc: 0.0433 - val_loss: 0.3727 - val_acc: 0.8760 - val_auc: 0.0419\n",
      "Epoch 187/200\n",
      "98600/98600 [==============================] - 7s 72us/sample - loss: 0.3835 - acc: 0.8533 - auc: 0.0440 - val_loss: 0.3634 - val_acc: 0.8660 - val_auc: 0.0551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/200\n",
      "98600/98600 [==============================] - 8s 77us/sample - loss: 0.3833 - acc: 0.8546 - auc: 0.0429 - val_loss: 0.3562 - val_acc: 0.8680 - val_auc: 0.0503\n",
      "Epoch 189/200\n",
      "98600/98600 [==============================] - 7s 71us/sample - loss: 0.3849 - acc: 0.8535 - auc: 0.0428 - val_loss: 0.3663 - val_acc: 0.8700 - val_auc: 0.0490\n",
      "Epoch 190/200\n",
      "98600/98600 [==============================] - 7s 70us/sample - loss: 0.3852 - acc: 0.8533 - auc: 0.0425 - val_loss: 0.3656 - val_acc: 0.8640 - val_auc: 0.0424\n",
      "Epoch 191/200\n",
      "98600/98600 [==============================] - 7s 66us/sample - loss: 0.3836 - acc: 0.8530 - auc: 0.0429 - val_loss: 0.3570 - val_acc: 0.8800 - val_auc: 0.0449\n",
      "Epoch 192/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3834 - acc: 0.8532 - auc: 0.0424 - val_loss: 0.3901 - val_acc: 0.8660 - val_auc: 0.0452\n",
      "Epoch 193/200\n",
      "98600/98600 [==============================] - 7s 76us/sample - loss: 0.3852 - acc: 0.8530 - auc: 0.0435 - val_loss: 0.3748 - val_acc: 0.8740 - val_auc: 0.0367\n",
      "Epoch 194/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3836 - acc: 0.8542 - auc: 0.0425 - val_loss: 0.3650 - val_acc: 0.8660 - val_auc: 0.0355\n",
      "Epoch 195/200\n",
      "98600/98600 [==============================] - 7s 68us/sample - loss: 0.3816 - acc: 0.8542 - auc: 0.0446 - val_loss: 0.3574 - val_acc: 0.8700 - val_auc: 0.0321\n",
      "Epoch 196/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3831 - acc: 0.8542 - auc: 0.0432 - val_loss: 0.3934 - val_acc: 0.8680 - val_auc: 0.0489\n",
      "Epoch 197/200\n",
      "98600/98600 [==============================] - 7s 68us/sample - loss: 0.3818 - acc: 0.8546 - auc: 0.0425 - val_loss: 0.3646 - val_acc: 0.8700 - val_auc: 0.0469\n",
      "Epoch 198/200\n",
      "98600/98600 [==============================] - 7s 69us/sample - loss: 0.3830 - acc: 0.8535 - auc: 0.0447 - val_loss: 0.3759 - val_acc: 0.8680 - val_auc: 0.0422\n",
      "Epoch 199/200\n",
      "98600/98600 [==============================] - 7s 73us/sample - loss: 0.3816 - acc: 0.8540 - auc: 0.0430 - val_loss: 0.3715 - val_acc: 0.8620 - val_auc: 0.0584\n",
      "Epoch 200/200\n",
      "98600/98600 [==============================] - 7s 69us/sample - loss: 0.3818 - acc: 0.8540 - auc: 0.0426 - val_loss: 0.3743 - val_acc: 0.8600 - val_auc: 0.0439\n"
     ]
    }
   ],
   "source": [
    "hist = tf_model.fit(\n",
    "X_train,\n",
    "        tf.keras.utils.to_categorical(y_train),\n",
    "        epochs=200,\n",
    "        batch_size=100,\n",
    "        validation_data=(X_val, tf.keras.utils.to_categorical(y_val)),\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:27:38.133274Z",
     "start_time": "2020-06-12T11:27:38.122856Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu_model(input):\n",
    "\n",
    "    \n",
    "    y = Dense(\n",
    "        200 ,\n",
    "        use_bias = True,\n",
    "        input_shape=(feature_dim,),kernel_regularizer='l2',\n",
    "        name=\"fc1\"\n",
    "    )(input)\n",
    "    y = Activation('sigmoid')(y)\n",
    "    y = Dropout(rate=0.3)(y, training=True)\n",
    "    y = Dense(\n",
    "        50 ,\n",
    "        use_bias = True,\n",
    "        kernel_regularizer='l2',name=\"fc13\"\n",
    "    )(y)\n",
    "    y = Activation('sigmoid')(y)\n",
    "    y = Dropout(rate=0.3)(y, training=True)\n",
    "    y = Dense(2, use_bias=True, name=\"fc_2\", kernel_regularizer='l2')(y)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:27:41.649484Z",
     "start_time": "2020-06-12T11:27:41.484742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 688)]             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 200)               137800    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "fc13 (Dense)                 (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "fc_2 (Dense)                 (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 147,952\n",
      "Trainable params: 147,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "feature_dim = X_train.shape[1]\n",
    "\n",
    "x_n = Input(shape=(feature_dim,), name=\"input\")\n",
    "\n",
    "y_n = relu_model(x_n)\n",
    "n_model = Model(inputs=x_n, outputs=y_n)\n",
    "print(n_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:47:33.800466Z",
     "start_time": "2020-06-12T11:27:56.272574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98600 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "98600/98600 [==============================] - 12s 117us/sample - loss: 0.8249 - acc: 0.8407 - auc_8: 0.0107 - val_loss: 0.4977 - val_acc: 0.8600 - val_auc_8: 0.0067\n",
      "Epoch 2/100\n",
      "98600/98600 [==============================] - 11s 114us/sample - loss: 0.4894 - acc: 0.8407 - auc_8: 0.0044 - val_loss: 0.4381 - val_acc: 0.8600 - val_auc_8: 0.0051\n",
      "Epoch 3/100\n",
      "98600/98600 [==============================] - 12s 124us/sample - loss: 0.4628 - acc: 0.8407 - auc_8: 0.0023 - val_loss: 0.4274 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 4/100\n",
      "98600/98600 [==============================] - 12s 122us/sample - loss: 0.4550 - acc: 0.8407 - auc_8: 7.6484e-04 - val_loss: 0.4211 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 5/100\n",
      "98600/98600 [==============================] - 13s 127us/sample - loss: 0.4506 - acc: 0.8407 - auc_8: 2.8956e-04 - val_loss: 0.4149 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 6/100\n",
      "98600/98600 [==============================] - 13s 128us/sample - loss: 0.4477 - acc: 0.8407 - auc_8: 1.0227e-04 - val_loss: 0.4153 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 7/100\n",
      "98600/98600 [==============================] - 13s 129us/sample - loss: 0.4462 - acc: 0.8407 - auc_8: 2.5580e-05 - val_loss: 0.4108 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 8/100\n",
      "98600/98600 [==============================] - 12s 127us/sample - loss: 0.4453 - acc: 0.8407 - auc_8: 8.5314e-06 - val_loss: 0.4135 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 9/100\n",
      "98600/98600 [==============================] - 13s 128us/sample - loss: 0.4441 - acc: 0.8407 - auc_8: 8.5313e-06 - val_loss: 0.4140 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 10/100\n",
      "98600/98600 [==============================] - 13s 127us/sample - loss: 0.4435 - acc: 0.8407 - auc_8: 8.5311e-06 - val_loss: 0.4093 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 11/100\n",
      "98600/98600 [==============================] - 13s 128us/sample - loss: 0.4433 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4126 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 12/100\n",
      "98600/98600 [==============================] - 13s 132us/sample - loss: 0.4430 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4119 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 13/100\n",
      "98600/98600 [==============================] - 13s 130us/sample - loss: 0.4428 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4117 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 14/100\n",
      "98600/98600 [==============================] - 12s 124us/sample - loss: 0.4426 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4110 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 15/100\n",
      "98600/98600 [==============================] - 14s 140us/sample - loss: 0.4426 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4113 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 16/100\n",
      "98600/98600 [==============================] - 14s 137us/sample - loss: 0.4424 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4116 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 17/100\n",
      "98600/98600 [==============================] - 15s 151us/sample - loss: 0.4425 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4107 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 18/100\n",
      "98600/98600 [==============================] - 13s 135us/sample - loss: 0.4422 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4133 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 19/100\n",
      "98600/98600 [==============================] - 14s 138us/sample - loss: 0.4419 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4085 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 20/100\n",
      "98600/98600 [==============================] - 12s 124us/sample - loss: 0.4419 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4115 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 21/100\n",
      "98600/98600 [==============================] - 14s 138us/sample - loss: 0.4419 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4086 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 22/100\n",
      "98600/98600 [==============================] - 15s 154us/sample - loss: 0.4417 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4089 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 23/100\n",
      "98600/98600 [==============================] - 15s 155us/sample - loss: 0.4416 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4103 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 24/100\n",
      "98600/98600 [==============================] - 15s 157us/sample - loss: 0.4417 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4132 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 25/100\n",
      "98600/98600 [==============================] - 14s 142us/sample - loss: 0.4417 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4077 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 26/100\n",
      "98600/98600 [==============================] - 17s 173us/sample - loss: 0.4415 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4107 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 27/100\n",
      "98600/98600 [==============================] - 13s 132us/sample - loss: 0.4415 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4119 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 28/100\n",
      "98600/98600 [==============================] - 19s 193us/sample - loss: 0.4413 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4118 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 29/100\n",
      "98600/98600 [==============================] - 14s 143us/sample - loss: 0.4414 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4103 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 30/100\n",
      "98600/98600 [==============================] - 16s 167us/sample - loss: 0.4411 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4087 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 31/100\n",
      "98600/98600 [==============================] - 19s 198us/sample - loss: 0.4413 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4068 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 32/100\n",
      "98600/98600 [==============================] - 14s 147us/sample - loss: 0.4415 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4103 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 33/100\n",
      "98600/98600 [==============================] - 17s 174us/sample - loss: 0.4413 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4095 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 34/100\n",
      "98600/98600 [==============================] - 17s 174us/sample - loss: 0.4411 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4094 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 35/100\n",
      "98600/98600 [==============================] - 14s 138us/sample - loss: 0.4411 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4095 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 36/100\n",
      "98600/98600 [==============================] - 14s 145us/sample - loss: 0.4408 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4084 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 37/100\n",
      "98600/98600 [==============================] - 16s 165us/sample - loss: 0.4408 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4130 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 38/100\n",
      "98600/98600 [==============================] - 11s 115us/sample - loss: 0.4411 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4106 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 39/100\n",
      "98600/98600 [==============================] - 13s 130us/sample - loss: 0.4408 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4097 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 40/100\n",
      "98600/98600 [==============================] - 13s 131us/sample - loss: 0.4408 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4091 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 41/100\n",
      "98600/98600 [==============================] - 13s 134us/sample - loss: 0.4406 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4094 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 42/100\n",
      "98600/98600 [==============================] - 13s 135us/sample - loss: 0.4405 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4093 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 43/100\n",
      "98600/98600 [==============================] - 14s 138us/sample - loss: 0.4405 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4065 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 44/100\n",
      "98600/98600 [==============================] - 13s 137us/sample - loss: 0.4406 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4108 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "98600/98600 [==============================] - 11s 111us/sample - loss: 0.4406 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4077 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 46/100\n",
      "98600/98600 [==============================] - 12s 124us/sample - loss: 0.4406 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4108 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 47/100\n",
      "98600/98600 [==============================] - 11s 116us/sample - loss: 0.4406 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4091 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 48/100\n",
      "98600/98600 [==============================] - 12s 119us/sample - loss: 0.4406 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4111 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 49/100\n",
      "98600/98600 [==============================] - 11s 109us/sample - loss: 0.4404 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4079 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 50/100\n",
      "98600/98600 [==============================] - 10s 100us/sample - loss: 0.4403 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4103 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 51/100\n",
      "98600/98600 [==============================] - 13s 127us/sample - loss: 0.4404 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4075 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 52/100\n",
      "98600/98600 [==============================] - 13s 130us/sample - loss: 0.4402 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4110 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 53/100\n",
      "98600/98600 [==============================] - 12s 126us/sample - loss: 0.4402 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4062 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 54/100\n",
      "98600/98600 [==============================] - 12s 127us/sample - loss: 0.4404 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4113 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 55/100\n",
      "98600/98600 [==============================] - 12s 123us/sample - loss: 0.4400 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4100 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 56/100\n",
      "98600/98600 [==============================] - 13s 130us/sample - loss: 0.4403 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4084 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 57/100\n",
      "98600/98600 [==============================] - 12s 125us/sample - loss: 0.4402 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4132 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 58/100\n",
      "98600/98600 [==============================] - 12s 118us/sample - loss: 0.4404 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4080 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 59/100\n",
      "98600/98600 [==============================] - 12s 123us/sample - loss: 0.4402 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4081 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 60/100\n",
      "98600/98600 [==============================] - 14s 139us/sample - loss: 0.4402 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4080 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 61/100\n",
      "98600/98600 [==============================] - 17s 175us/sample - loss: 0.4401 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4092 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 62/100\n",
      "98600/98600 [==============================] - 13s 132us/sample - loss: 0.4402 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4102 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 63/100\n",
      "98600/98600 [==============================] - 11s 116us/sample - loss: 0.4401 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4079 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 64/100\n",
      "98600/98600 [==============================] - 11s 107us/sample - loss: 0.4402 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4069 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 65/100\n",
      "98600/98600 [==============================] - 9s 87us/sample - loss: 0.4400 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4082 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 66/100\n",
      "98600/98600 [==============================] - 9s 87us/sample - loss: 0.4399 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4091 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 67/100\n",
      "98600/98600 [==============================] - 9s 91us/sample - loss: 0.4399 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4063 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 68/100\n",
      "98600/98600 [==============================] - 10s 97us/sample - loss: 0.4397 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4068 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 69/100\n",
      "98600/98600 [==============================] - 9s 88us/sample - loss: 0.4400 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4075 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 70/100\n",
      "98600/98600 [==============================] - 9s 94us/sample - loss: 0.4400 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4093 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 71/100\n",
      "98600/98600 [==============================] - 8s 84us/sample - loss: 0.4397 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4089 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 72/100\n",
      "98600/98600 [==============================] - 8s 86us/sample - loss: 0.4397 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4083 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 73/100\n",
      "98600/98600 [==============================] - 8s 80us/sample - loss: 0.4396 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4062 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 74/100\n",
      "98600/98600 [==============================] - 8s 83us/sample - loss: 0.4398 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4095 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 75/100\n",
      "98600/98600 [==============================] - 8s 84us/sample - loss: 0.4395 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4068 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 76/100\n",
      "98600/98600 [==============================] - 9s 92us/sample - loss: 0.4397 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4072 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 77/100\n",
      "98600/98600 [==============================] - 10s 98us/sample - loss: 0.4396 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4090 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 78/100\n",
      "98600/98600 [==============================] - 8s 80us/sample - loss: 0.4397 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4092 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 79/100\n",
      "98600/98600 [==============================] - 8s 85us/sample - loss: 0.4397 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4085 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 80/100\n",
      "98600/98600 [==============================] - 9s 92us/sample - loss: 0.4396 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4057 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 81/100\n",
      "98600/98600 [==============================] - 7s 72us/sample - loss: 0.4396 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4079 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 82/100\n",
      "98600/98600 [==============================] - 10s 99us/sample - loss: 0.4396 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4085 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 83/100\n",
      "98600/98600 [==============================] - 9s 94us/sample - loss: 0.4395 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4074 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 84/100\n",
      "98600/98600 [==============================] - 10s 101us/sample - loss: 0.4394 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4073 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 85/100\n",
      "98600/98600 [==============================] - 9s 92us/sample - loss: 0.4395 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4091 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 86/100\n",
      "98600/98600 [==============================] - 8s 86us/sample - loss: 0.4395 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4074 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 87/100\n",
      "98600/98600 [==============================] - 9s 88us/sample - loss: 0.4395 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4074 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 88/100\n",
      "98600/98600 [==============================] - 8s 84us/sample - loss: 0.4395 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4081 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98600/98600 [==============================] - 9s 86us/sample - loss: 0.4393 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4094 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 90/100\n",
      "98600/98600 [==============================] - 6s 62us/sample - loss: 0.4394 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4074 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 91/100\n",
      "98600/98600 [==============================] - 6s 63us/sample - loss: 0.4393 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4063 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 92/100\n",
      "98600/98600 [==============================] - 10s 97us/sample - loss: 0.4392 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4071 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 93/100\n",
      "98600/98600 [==============================] - 15s 150us/sample - loss: 0.4393 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4083 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 94/100\n",
      "98600/98600 [==============================] - 10s 104us/sample - loss: 0.4393 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4083 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 95/100\n",
      "98600/98600 [==============================] - 10s 105us/sample - loss: 0.4393 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4067 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 96/100\n",
      "98600/98600 [==============================] - 10s 100us/sample - loss: 0.4393 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4083 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 97/100\n",
      "98600/98600 [==============================] - 8s 79us/sample - loss: 0.4392 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4080 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 98/100\n",
      "98600/98600 [==============================] - 12s 122us/sample - loss: 0.4393 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4066 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 99/100\n",
      "98600/98600 [==============================] - 7s 70us/sample - loss: 0.4394 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4072 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n",
      "Epoch 100/100\n",
      "98600/98600 [==============================] - 7s 70us/sample - loss: 0.4394 - acc: 0.8407 - auc_8: 0.0000e+00 - val_loss: 0.4050 - val_acc: 0.8600 - val_auc_8: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "n_model.compile(\n",
    "    optimizer=optimizer, loss=lossf, metrics=[\"accuracy\",\"AUC\"])\n",
    "n_hist = n_model.fit(\n",
    "X_train,\n",
    "        tf.keras.utils.to_categorical(y_train),\n",
    "        epochs=100,\n",
    "        batch_size=100,\n",
    "        validation_data=(X_val, tf.keras.utils.to_categorical(y_val)),\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T12:18:18.987613Z",
     "start_time": "2020-06-12T12:18:18.192868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.gca()\n",
    "plt.plot(hist.history['val_loss'][:100], label='encrypted')\n",
    "plt.plot(n_hist.history['val_loss'], label='non-encrypted')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.legend()\n",
    "plt.savefig('loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T12:10:26.613597Z",
     "start_time": "2020-06-12T12:10:26.572150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [28382.495664399245,\n",
       "  13402.723376786733,\n",
       "  6341.184208310884,\n",
       "  3221.715593318668,\n",
       "  1714.2715883699923,\n",
       "  998.2361781679351,\n",
       "  617.0111411290043,\n",
       "  392.0113632325952,\n",
       "  261.9731094484155,\n",
       "  175.48455197332356,\n",
       "  112.27682571237034,\n",
       "  69.53902961467874,\n",
       "  40.90486662092847,\n",
       "  23.511705179968907,\n",
       "  13.394550456962044,\n",
       "  7.538621632612743,\n",
       "  4.576073684266808,\n",
       "  3.1472293292049462,\n",
       "  2.3727883877908966,\n",
       "  1.9639943223937772,\n",
       "  1.7632990414182264,\n",
       "  1.649509895041313,\n",
       "  1.5747308303086318,\n",
       "  7.116095178630241,\n",
       "  1.4621233458934884,\n",
       "  1.3773454124255307,\n",
       "  1.355779785897137,\n",
       "  1.3317802570170854,\n",
       "  1.2978579688507452,\n",
       "  1.2521996321591111,\n",
       "  1.191371603258725,\n",
       "  1.1295948304825332,\n",
       "  7.103011220330883,\n",
       "  0.9602428425035419,\n",
       "  0.9389504597467535,\n",
       "  0.9258473511278992,\n",
       "  0.911569877280183,\n",
       "  0.8917568485586929,\n",
       "  0.8650621751259113,\n",
       "  0.8287165415214477,\n",
       "  0.7852614307983894,\n",
       "  6.661339722770223,\n",
       "  0.7033261026862185,\n",
       "  0.6863213605861394,\n",
       "  0.6801413722018925,\n",
       "  0.6706184506053615,\n",
       "  0.6591784193900245,\n",
       "  0.6446537877193087,\n",
       "  0.627140934550738,\n",
       "  0.6063251255673279,\n",
       "  0.5805820229994114,\n",
       "  2.571863482345432,\n",
       "  0.534073577933089,\n",
       "  0.5275823043821791,\n",
       "  0.5224264101795928,\n",
       "  0.5164180664878346,\n",
       "  0.5095200705177644,\n",
       "  0.5000915261661789,\n",
       "  0.4913029338235062,\n",
       "  1.0634482349391883,\n",
       "  0.45116920170750147,\n",
       "  0.4468439554164666,\n",
       "  0.4446299401306477,\n",
       "  0.44353812766728007,\n",
       "  0.4398286870962224,\n",
       "  0.43327957174115445,\n",
       "  0.4305461007368734,\n",
       "  0.4251644044177527,\n",
       "  0.41150797942830875,\n",
       "  0.4093400550209001,\n",
       "  0.4019189002729091,\n",
       "  0.39971197435809924,\n",
       "  0.3962082215424242,\n",
       "  0.3925146739816569,\n",
       "  0.39227898347873474,\n",
       "  0.3921629965275103,\n",
       "  0.3954141842877647,\n",
       "  0.39048433284186446,\n",
       "  0.3898347821577809,\n",
       "  0.3898580387144011,\n",
       "  0.39078847758668434,\n",
       "  0.38828330973643077,\n",
       "  0.388784939494989,\n",
       "  0.3887829222918524,\n",
       "  0.39007660064989846,\n",
       "  0.38862493032736906,\n",
       "  0.3875988030893082,\n",
       "  0.38981231921645254,\n",
       "  0.3898155501690404,\n",
       "  0.3882172142758089,\n",
       "  0.38653449670356865,\n",
       "  0.3894545385580034,\n",
       "  0.38712437351141693,\n",
       "  0.38874484844974283,\n",
       "  0.38908887279746984,\n",
       "  0.3892298437670085,\n",
       "  0.38860589850686383,\n",
       "  0.3887405792972863,\n",
       "  0.3892435618727251,\n",
       "  0.3876161927595342,\n",
       "  0.38826330229791134,\n",
       "  0.3899975981238406,\n",
       "  0.390022429231456,\n",
       "  0.3902572995357775,\n",
       "  0.3894625416754709,\n",
       "  0.3892654146371942,\n",
       "  0.38695748899940546,\n",
       "  0.38664561741310977,\n",
       "  0.3882024550534649,\n",
       "  0.3899127645134684,\n",
       "  0.389465374392259,\n",
       "  0.3890597005856448,\n",
       "  0.3885963686853345,\n",
       "  0.3883675268679313,\n",
       "  0.3884903309643873,\n",
       "  0.38856308323918687,\n",
       "  0.3884149293961196,\n",
       "  0.3897124465562509,\n",
       "  0.38777927641754945,\n",
       "  0.38831426012830367,\n",
       "  0.3888077034972019,\n",
       "  0.38794218401214897,\n",
       "  0.38698211587345865,\n",
       "  0.38778898414624874,\n",
       "  0.3877656634360491,\n",
       "  0.38824094407275533,\n",
       "  0.38925381464237613,\n",
       "  0.3877024077952029,\n",
       "  0.38730577069653943,\n",
       "  0.3880077846514526,\n",
       "  0.3883257288085278,\n",
       "  0.3897623308683022,\n",
       "  0.38877834001854517,\n",
       "  0.3859640923130343,\n",
       "  0.3884873173632196,\n",
       "  0.3875514367712439,\n",
       "  0.38737399164307673,\n",
       "  0.38694710645665986,\n",
       "  0.38706596173028673,\n",
       "  0.38748772841317175,\n",
       "  0.3859473590344251,\n",
       "  0.38405371014535067,\n",
       "  0.38565631410292145,\n",
       "  0.3860242605420929,\n",
       "  0.3864357003877903,\n",
       "  0.38500056504961444,\n",
       "  0.3845963047073047,\n",
       "  0.38495477015420826,\n",
       "  0.3827231602845279,\n",
       "  0.3850189540346302,\n",
       "  0.38382082183148264,\n",
       "  0.383869373366146,\n",
       "  0.38374962603284185,\n",
       "  0.38482176742193425,\n",
       "  0.3829617376380712,\n",
       "  0.38221903987092853,\n",
       "  0.38480263892523414,\n",
       "  0.38290186292864004,\n",
       "  0.38349520599443093,\n",
       "  0.38342266090017785,\n",
       "  0.3857809766129596,\n",
       "  0.3847865146900287,\n",
       "  0.3831350068140465,\n",
       "  0.38326457002281417,\n",
       "  0.38305987930019775,\n",
       "  0.38508120158018494,\n",
       "  0.38420116266662646,\n",
       "  0.3844289679163368,\n",
       "  0.3841475969667851,\n",
       "  0.3840441790241257,\n",
       "  0.3836656825930545,\n",
       "  0.3839411514284161,\n",
       "  0.3830350308280446,\n",
       "  0.3831841045382299,\n",
       "  0.38314576348777235,\n",
       "  0.38472040366328514,\n",
       "  0.3836937840370571,\n",
       "  0.3843313968356918,\n",
       "  0.382941163804299,\n",
       "  0.3813830483062514,\n",
       "  0.3832129339477837,\n",
       "  0.3843270472381226,\n",
       "  0.3845637712257387,\n",
       "  0.3833199014649188,\n",
       "  0.3832704620719198,\n",
       "  0.38396646380122246,\n",
       "  0.3834708348279309,\n",
       "  0.38329233879008107,\n",
       "  0.3849269310069133,\n",
       "  0.38524697329584534,\n",
       "  0.3835730124076046,\n",
       "  0.3833855962263643,\n",
       "  0.38516457471052,\n",
       "  0.3836449857113569,\n",
       "  0.38156621975722227,\n",
       "  0.38308888750857323,\n",
       "  0.3818299680283297,\n",
       "  0.3830287148665947,\n",
       "  0.38162226483800105,\n",
       "  0.3818119843518758],\n",
       " 'acc': [0.7558925,\n",
       "  0.7558519,\n",
       "  0.7559939,\n",
       "  0.75619674,\n",
       "  0.7550507,\n",
       "  0.7562779,\n",
       "  0.7571501,\n",
       "  0.75865114,\n",
       "  0.75869167,\n",
       "  0.76674443,\n",
       "  0.7676876,\n",
       "  0.77090263,\n",
       "  0.7757809,\n",
       "  0.77954364,\n",
       "  0.78496957,\n",
       "  0.7892596,\n",
       "  0.7981136,\n",
       "  0.80418867,\n",
       "  0.8125862,\n",
       "  0.8242292,\n",
       "  0.829716,\n",
       "  0.83404666,\n",
       "  0.83377284,\n",
       "  0.8223428,\n",
       "  0.8379209,\n",
       "  0.8436714,\n",
       "  0.8440669,\n",
       "  0.84576064,\n",
       "  0.84556794,\n",
       "  0.8458925,\n",
       "  0.8453245,\n",
       "  0.84198785,\n",
       "  0.8322312,\n",
       "  0.84850913,\n",
       "  0.85103446,\n",
       "  0.8521298,\n",
       "  0.8521907,\n",
       "  0.85130835,\n",
       "  0.850284,\n",
       "  0.8499797,\n",
       "  0.84808314,\n",
       "  0.8344827,\n",
       "  0.85034484,\n",
       "  0.8533874,\n",
       "  0.85283977,\n",
       "  0.85357,\n",
       "  0.85271806,\n",
       "  0.8534077,\n",
       "  0.85198784,\n",
       "  0.8505578,\n",
       "  0.8478702,\n",
       "  0.8418053,\n",
       "  0.8537221,\n",
       "  0.855213,\n",
       "  0.854716,\n",
       "  0.85357,\n",
       "  0.8514503,\n",
       "  0.8507911,\n",
       "  0.8496146,\n",
       "  0.8386511,\n",
       "  0.8522414,\n",
       "  0.8527789,\n",
       "  0.8526166,\n",
       "  0.8510953,\n",
       "  0.85061866,\n",
       "  0.85114604,\n",
       "  0.8492495,\n",
       "  0.8494016,\n",
       "  0.8496045,\n",
       "  0.8486004,\n",
       "  0.84893507,\n",
       "  0.8494118,\n",
       "  0.84889454,\n",
       "  0.8491075,\n",
       "  0.8494118,\n",
       "  0.84991884,\n",
       "  0.84998983,\n",
       "  0.8498986,\n",
       "  0.84973633,\n",
       "  0.84962475,\n",
       "  0.849858,\n",
       "  0.8504868,\n",
       "  0.8494118,\n",
       "  0.8505071,\n",
       "  0.8490771,\n",
       "  0.8500304,\n",
       "  0.8505274,\n",
       "  0.84954363,\n",
       "  0.8509838,\n",
       "  0.8503347,\n",
       "  0.85035497,\n",
       "  0.8497059,\n",
       "  0.8507911,\n",
       "  0.85039556,\n",
       "  0.850071,\n",
       "  0.850142,\n",
       "  0.85034484,\n",
       "  0.85030425,\n",
       "  0.84973633,\n",
       "  0.8506085,\n",
       "  0.84996957,\n",
       "  0.84916836,\n",
       "  0.84981745,\n",
       "  0.8500304,\n",
       "  0.850142,\n",
       "  0.8500507,\n",
       "  0.8509432,\n",
       "  0.85073024,\n",
       "  0.85049695,\n",
       "  0.84981745,\n",
       "  0.8497566,\n",
       "  0.8508925,\n",
       "  0.8494118,\n",
       "  0.85035497,\n",
       "  0.84976673,\n",
       "  0.85042596,\n",
       "  0.84969574,\n",
       "  0.8488844,\n",
       "  0.8508012,\n",
       "  0.8491075,\n",
       "  0.84991884,\n",
       "  0.84984785,\n",
       "  0.8506085,\n",
       "  0.85081136,\n",
       "  0.850213,\n",
       "  0.8502535,\n",
       "  0.8495639,\n",
       "  0.8507201,\n",
       "  0.8507201,\n",
       "  0.84993917,\n",
       "  0.8493509,\n",
       "  0.8489554,\n",
       "  0.85042596,\n",
       "  0.85129815,\n",
       "  0.85064906,\n",
       "  0.8506694,\n",
       "  0.85001016,\n",
       "  0.8512576,\n",
       "  0.8515416,\n",
       "  0.8507404,\n",
       "  0.8517039,\n",
       "  0.8525456,\n",
       "  0.85249496,\n",
       "  0.85182554,\n",
       "  0.8520487,\n",
       "  0.85301214,\n",
       "  0.8526268,\n",
       "  0.8531542,\n",
       "  0.85392493,\n",
       "  0.8534787,\n",
       "  0.8547972,\n",
       "  0.8539858,\n",
       "  0.8525152,\n",
       "  0.8536106,\n",
       "  0.8540365,\n",
       "  0.8529716,\n",
       "  0.8528905,\n",
       "  0.85339755,\n",
       "  0.85432047,\n",
       "  0.8531643,\n",
       "  0.8519067,\n",
       "  0.85287017,\n",
       "  0.8529716,\n",
       "  0.8534077,\n",
       "  0.8542292,\n",
       "  0.85275865,\n",
       "  0.85408723,\n",
       "  0.85226166,\n",
       "  0.8535091,\n",
       "  0.8524341,\n",
       "  0.8536714,\n",
       "  0.853428,\n",
       "  0.853073,\n",
       "  0.85393506,\n",
       "  0.8547363,\n",
       "  0.8534584,\n",
       "  0.8538337,\n",
       "  0.85378295,\n",
       "  0.85427994,\n",
       "  0.8546146,\n",
       "  0.85363084,\n",
       "  0.8535091,\n",
       "  0.85368156,\n",
       "  0.8539554,\n",
       "  0.85324544,\n",
       "  0.8531846,\n",
       "  0.85332656,\n",
       "  0.854574,\n",
       "  0.85348886,\n",
       "  0.8533164,\n",
       "  0.85301214,\n",
       "  0.8531947,\n",
       "  0.85301214,\n",
       "  0.8541582,\n",
       "  0.8542495,\n",
       "  0.8541785,\n",
       "  0.854645,\n",
       "  0.85348886,\n",
       "  0.8539858,\n",
       "  0.85401624],\n",
       " 'auc': [6.568093e-05,\n",
       "  6.559598e-05,\n",
       "  0.00013065577,\n",
       "  0.0002234404,\n",
       "  0.0002505127,\n",
       "  0.00046597855,\n",
       "  0.0007096411,\n",
       "  0.0010522053,\n",
       "  0.0018639418,\n",
       "  0.0030914615,\n",
       "  0.0046573128,\n",
       "  0.006443025,\n",
       "  0.008796915,\n",
       "  0.011651868,\n",
       "  0.015421018,\n",
       "  0.020636726,\n",
       "  0.02615756,\n",
       "  0.032664202,\n",
       "  0.03721368,\n",
       "  0.038977258,\n",
       "  0.03847739,\n",
       "  0.03660207,\n",
       "  0.036698572,\n",
       "  0.03450502,\n",
       "  0.031485137,\n",
       "  0.034573615,\n",
       "  0.036169656,\n",
       "  0.037354004,\n",
       "  0.037786804,\n",
       "  0.037903942,\n",
       "  0.039068487,\n",
       "  0.038774215,\n",
       "  0.03593961,\n",
       "  0.039465733,\n",
       "  0.0422251,\n",
       "  0.04258411,\n",
       "  0.04363998,\n",
       "  0.0440266,\n",
       "  0.04284192,\n",
       "  0.042277843,\n",
       "  0.04136811,\n",
       "  0.036272865,\n",
       "  0.0408509,\n",
       "  0.043244697,\n",
       "  0.044099838,\n",
       "  0.04462469,\n",
       "  0.045677222,\n",
       "  0.04499051,\n",
       "  0.04383781,\n",
       "  0.043172512,\n",
       "  0.043805532,\n",
       "  0.04011115,\n",
       "  0.045160055,\n",
       "  0.04656922,\n",
       "  0.046268012,\n",
       "  0.04663126,\n",
       "  0.047111418,\n",
       "  0.04553689,\n",
       "  0.044772174,\n",
       "  0.041236546,\n",
       "  0.045270707,\n",
       "  0.04612755,\n",
       "  0.04635913,\n",
       "  0.04771561,\n",
       "  0.046742048,\n",
       "  0.044606633,\n",
       "  0.045530654,\n",
       "  0.04436283,\n",
       "  0.04569102,\n",
       "  0.045949,\n",
       "  0.04523149,\n",
       "  0.044136427,\n",
       "  0.04510755,\n",
       "  0.046243425,\n",
       "  0.045954682,\n",
       "  0.045015924,\n",
       "  0.045148406,\n",
       "  0.04430706,\n",
       "  0.04426161,\n",
       "  0.045520782,\n",
       "  0.04317373,\n",
       "  0.04439152,\n",
       "  0.044702984,\n",
       "  0.044219423,\n",
       "  0.044192966,\n",
       "  0.0448675,\n",
       "  0.045923762,\n",
       "  0.04459317,\n",
       "  0.04357483,\n",
       "  0.04561302,\n",
       "  0.045345865,\n",
       "  0.045656417,\n",
       "  0.044852156,\n",
       "  0.043233,\n",
       "  0.045693126,\n",
       "  0.04599005,\n",
       "  0.046114396,\n",
       "  0.045164224,\n",
       "  0.045755744,\n",
       "  0.0451547,\n",
       "  0.046251446,\n",
       "  0.04515184,\n",
       "  0.045269664,\n",
       "  0.045823816,\n",
       "  0.045390435,\n",
       "  0.045456845,\n",
       "  0.046395544,\n",
       "  0.04622113,\n",
       "  0.04543161,\n",
       "  0.045362275,\n",
       "  0.04512864,\n",
       "  0.045648895,\n",
       "  0.04583643,\n",
       "  0.04673528,\n",
       "  0.04574692,\n",
       "  0.04550375,\n",
       "  0.046416834,\n",
       "  0.04668467,\n",
       "  0.045407698,\n",
       "  0.04638213,\n",
       "  0.047969162,\n",
       "  0.04658482,\n",
       "  0.045786098,\n",
       "  0.046002947,\n",
       "  0.047472596,\n",
       "  0.04563694,\n",
       "  0.04606482,\n",
       "  0.04656215,\n",
       "  0.046352655,\n",
       "  0.04581044,\n",
       "  0.04722056,\n",
       "  0.04666657,\n",
       "  0.04498892,\n",
       "  0.046342686,\n",
       "  0.045604672,\n",
       "  0.046436332,\n",
       "  0.04691491,\n",
       "  0.04598165,\n",
       "  0.045781054,\n",
       "  0.046757355,\n",
       "  0.045323744,\n",
       "  0.045056812,\n",
       "  0.044892136,\n",
       "  0.046152752,\n",
       "  0.044745732,\n",
       "  0.04455751,\n",
       "  0.045378547,\n",
       "  0.04426353,\n",
       "  0.044164453,\n",
       "  0.04372531,\n",
       "  0.04268327,\n",
       "  0.042801335,\n",
       "  0.044486247,\n",
       "  0.04422635,\n",
       "  0.044879884,\n",
       "  0.04469893,\n",
       "  0.043557514,\n",
       "  0.0448681,\n",
       "  0.043463316,\n",
       "  0.043969095,\n",
       "  0.041945443,\n",
       "  0.043901004,\n",
       "  0.044906884,\n",
       "  0.043703064,\n",
       "  0.043393154,\n",
       "  0.04371688,\n",
       "  0.043656867,\n",
       "  0.043393314,\n",
       "  0.04365939,\n",
       "  0.043710113,\n",
       "  0.044242654,\n",
       "  0.04390239,\n",
       "  0.043777376,\n",
       "  0.04375465,\n",
       "  0.042885326,\n",
       "  0.043334905,\n",
       "  0.042396855,\n",
       "  0.043790787,\n",
       "  0.044355758,\n",
       "  0.04390504,\n",
       "  0.043833494,\n",
       "  0.041765276,\n",
       "  0.042859174,\n",
       "  0.04284537,\n",
       "  0.043897215,\n",
       "  0.043330997,\n",
       "  0.04401346,\n",
       "  0.042893272,\n",
       "  0.042844087,\n",
       "  0.0425061,\n",
       "  0.042889956,\n",
       "  0.042389125,\n",
       "  0.04348495,\n",
       "  0.042506628,\n",
       "  0.044624586,\n",
       "  0.04317272,\n",
       "  0.04245893,\n",
       "  0.04465878,\n",
       "  0.042986948,\n",
       "  0.042559143],\n",
       " 'val_loss': [15546.85615234375,\n",
       "  7926.62333984375,\n",
       "  3649.045654296875,\n",
       "  1848.0924560546875,\n",
       "  984.8944580078125,\n",
       "  709.9583618164063,\n",
       "  518.6654968261719,\n",
       "  275.62742309570314,\n",
       "  174.32054443359374,\n",
       "  145.82443237304688,\n",
       "  85.11852111816407,\n",
       "  51.12750473022461,\n",
       "  26.55582160949707,\n",
       "  14.477832794189453,\n",
       "  10.83600549697876,\n",
       "  5.413878965377807,\n",
       "  3.7868230819702147,\n",
       "  2.5856671810150145,\n",
       "  2.09202663898468,\n",
       "  1.7698750495910645,\n",
       "  1.669471549987793,\n",
       "  1.5737178325653076,\n",
       "  1.4961756944656373,\n",
       "  4.334515047073364,\n",
       "  1.389816951751709,\n",
       "  1.332124614715576,\n",
       "  1.327997350692749,\n",
       "  1.3083673477172852,\n",
       "  1.2469003677368165,\n",
       "  1.1875099420547486,\n",
       "  1.1417576313018798,\n",
       "  1.0457279682159424,\n",
       "  0.9674421191215515,\n",
       "  0.9255416631698609,\n",
       "  0.9144997477531434,\n",
       "  0.9028036952018738,\n",
       "  0.8705690145492554,\n",
       "  0.8677927494049072,\n",
       "  0.8392724990844727,\n",
       "  0.7821486592292786,\n",
       "  0.745061206817627,\n",
       "  0.697895348072052,\n",
       "  0.6744709610939026,\n",
       "  0.6780017852783203,\n",
       "  0.6448920249938965,\n",
       "  0.6526670575141906,\n",
       "  0.6371209859848023,\n",
       "  0.6305614352226258,\n",
       "  0.5907460808753967,\n",
       "  0.5908442616462708,\n",
       "  0.6475614070892334,\n",
       "  0.5040127277374268,\n",
       "  0.5222815632820129,\n",
       "  0.5137215375900268,\n",
       "  0.508706933259964,\n",
       "  0.494128155708313,\n",
       "  0.5101695537567139,\n",
       "  0.48792616128921507,\n",
       "  0.4698088228702545,\n",
       "  0.43085684776306155,\n",
       "  0.42356244325637815,\n",
       "  0.43353736996650694,\n",
       "  0.4486208438873291,\n",
       "  0.42172116637229917,\n",
       "  0.452823680639267,\n",
       "  0.42600364685058595,\n",
       "  0.41626129746437074,\n",
       "  0.4151403844356537,\n",
       "  0.3928330779075623,\n",
       "  0.39821009039878846,\n",
       "  0.39085344672203065,\n",
       "  0.3821651577949524,\n",
       "  0.36815011501312256,\n",
       "  0.3576112985610962,\n",
       "  0.38593276143074035,\n",
       "  0.3722452461719513,\n",
       "  0.37523815631866453,\n",
       "  0.4048389375209808,\n",
       "  0.3796974003314972,\n",
       "  0.384772127866745,\n",
       "  0.3730067014694214,\n",
       "  0.388898104429245,\n",
       "  0.36885805130004884,\n",
       "  0.3888179033994675,\n",
       "  0.3655798077583313,\n",
       "  0.39250377416610716,\n",
       "  0.3900224447250366,\n",
       "  0.3569722890853882,\n",
       "  0.3767575085163116,\n",
       "  0.37011680006980896,\n",
       "  0.38196506202220915,\n",
       "  0.37675158977508544,\n",
       "  0.3646036684513092,\n",
       "  0.37679859399795534,\n",
       "  0.39083329439163206,\n",
       "  0.38757009506225587,\n",
       "  0.3719563722610474,\n",
       "  0.36114510893821716,\n",
       "  0.3604903519153595,\n",
       "  0.37014525532722475,\n",
       "  0.3652014255523682,\n",
       "  0.3650782346725464,\n",
       "  0.35880223512649534,\n",
       "  0.36857662796974183,\n",
       "  0.3913785755634308,\n",
       "  0.3877660036087036,\n",
       "  0.3918856382369995,\n",
       "  0.3477341592311859,\n",
       "  0.366077321767807,\n",
       "  0.39893685579299926,\n",
       "  0.4017833828926086,\n",
       "  0.39513677954673765,\n",
       "  0.37708540558815,\n",
       "  0.36380932331085203,\n",
       "  0.37224812507629396,\n",
       "  0.37389827370643614,\n",
       "  0.37437690496444703,\n",
       "  0.3813589155673981,\n",
       "  0.38777234554290774,\n",
       "  0.3729796826839447,\n",
       "  0.3606269180774689,\n",
       "  0.373887425661087,\n",
       "  0.3592828571796417,\n",
       "  0.37048056721687317,\n",
       "  0.3684233486652374,\n",
       "  0.3788363695144653,\n",
       "  0.37856934666633607,\n",
       "  0.3758396506309509,\n",
       "  0.3601581215858459,\n",
       "  0.3684676647186279,\n",
       "  0.3779771149158478,\n",
       "  0.3727336823940277,\n",
       "  0.3669786036014557,\n",
       "  0.3579054236412048,\n",
       "  0.3651022672653198,\n",
       "  0.374211972951889,\n",
       "  0.3714399039745331,\n",
       "  0.3613158524036407,\n",
       "  0.3788889169692993,\n",
       "  0.37536208629608153,\n",
       "  0.36223568916320803,\n",
       "  0.39695656299591064,\n",
       "  0.35640265941619875,\n",
       "  0.3977629065513611,\n",
       "  0.36636285185813905,\n",
       "  0.3614762008190155,\n",
       "  0.3671265184879303,\n",
       "  0.3598542928695679,\n",
       "  0.36078070998191836,\n",
       "  0.3892425298690796,\n",
       "  0.3705766975879669,\n",
       "  0.3809745848178864,\n",
       "  0.3743779182434082,\n",
       "  0.35853931307792664,\n",
       "  0.36197216510772706,\n",
       "  0.36924041509628297,\n",
       "  0.3850916922092438,\n",
       "  0.3833596229553223,\n",
       "  0.3731537938117981,\n",
       "  0.36247783303260805,\n",
       "  0.39579758644104,\n",
       "  0.3757375717163086,\n",
       "  0.36007326245307925,\n",
       "  0.3845932364463806,\n",
       "  0.37108696699142457,\n",
       "  0.3795148551464081,\n",
       "  0.3584351420402527,\n",
       "  0.36439611315727233,\n",
       "  0.3986374258995056,\n",
       "  0.3799149751663208,\n",
       "  0.3544352173805237,\n",
       "  0.37961122393608093,\n",
       "  0.36614684462547303,\n",
       "  0.3629388093948364,\n",
       "  0.370460045337677,\n",
       "  0.3778835415840149,\n",
       "  0.36763015389442444,\n",
       "  0.36826366782188413,\n",
       "  0.36378113031387327,\n",
       "  0.3587847352027893,\n",
       "  0.3698180913925171,\n",
       "  0.37415690422058107,\n",
       "  0.3690190315246582,\n",
       "  0.35794203281402587,\n",
       "  0.3711281597614288,\n",
       "  0.3726904273033142,\n",
       "  0.3634312510490417,\n",
       "  0.35624573230743406,\n",
       "  0.3663432002067566,\n",
       "  0.3655715584754944,\n",
       "  0.35701704621315,\n",
       "  0.3901146173477173,\n",
       "  0.3748382568359375,\n",
       "  0.36502051949501035,\n",
       "  0.3573892652988434,\n",
       "  0.3934068441390991,\n",
       "  0.3646234661340714,\n",
       "  0.3759297013282776,\n",
       "  0.37153376936912536,\n",
       "  0.3742533683776855],\n",
       " 'val_acc': [0.792,\n",
       "  0.77,\n",
       "  0.748,\n",
       "  0.77,\n",
       "  0.798,\n",
       "  0.762,\n",
       "  0.774,\n",
       "  0.792,\n",
       "  0.802,\n",
       "  0.772,\n",
       "  0.762,\n",
       "  0.794,\n",
       "  0.8,\n",
       "  0.802,\n",
       "  0.804,\n",
       "  0.808,\n",
       "  0.824,\n",
       "  0.83,\n",
       "  0.844,\n",
       "  0.848,\n",
       "  0.848,\n",
       "  0.854,\n",
       "  0.848,\n",
       "  0.736,\n",
       "  0.864,\n",
       "  0.868,\n",
       "  0.866,\n",
       "  0.862,\n",
       "  0.866,\n",
       "  0.864,\n",
       "  0.856,\n",
       "  0.86,\n",
       "  0.85,\n",
       "  0.872,\n",
       "  0.866,\n",
       "  0.864,\n",
       "  0.87,\n",
       "  0.858,\n",
       "  0.862,\n",
       "  0.862,\n",
       "  0.866,\n",
       "  0.87,\n",
       "  0.87,\n",
       "  0.866,\n",
       "  0.876,\n",
       "  0.856,\n",
       "  0.874,\n",
       "  0.866,\n",
       "  0.868,\n",
       "  0.864,\n",
       "  0.822,\n",
       "  0.878,\n",
       "  0.868,\n",
       "  0.868,\n",
       "  0.862,\n",
       "  0.87,\n",
       "  0.858,\n",
       "  0.862,\n",
       "  0.868,\n",
       "  0.868,\n",
       "  0.872,\n",
       "  0.866,\n",
       "  0.856,\n",
       "  0.874,\n",
       "  0.868,\n",
       "  0.878,\n",
       "  0.858,\n",
       "  0.866,\n",
       "  0.868,\n",
       "  0.858,\n",
       "  0.868,\n",
       "  0.866,\n",
       "  0.87,\n",
       "  0.866,\n",
       "  0.858,\n",
       "  0.868,\n",
       "  0.864,\n",
       "  0.852,\n",
       "  0.868,\n",
       "  0.86,\n",
       "  0.86,\n",
       "  0.86,\n",
       "  0.862,\n",
       "  0.86,\n",
       "  0.876,\n",
       "  0.87,\n",
       "  0.862,\n",
       "  0.864,\n",
       "  0.86,\n",
       "  0.862,\n",
       "  0.86,\n",
       "  0.858,\n",
       "  0.864,\n",
       "  0.868,\n",
       "  0.858,\n",
       "  0.858,\n",
       "  0.862,\n",
       "  0.876,\n",
       "  0.862,\n",
       "  0.866,\n",
       "  0.872,\n",
       "  0.854,\n",
       "  0.868,\n",
       "  0.868,\n",
       "  0.874,\n",
       "  0.85,\n",
       "  0.85,\n",
       "  0.878,\n",
       "  0.866,\n",
       "  0.858,\n",
       "  0.864,\n",
       "  0.864,\n",
       "  0.858,\n",
       "  0.866,\n",
       "  0.88,\n",
       "  0.864,\n",
       "  0.87,\n",
       "  0.858,\n",
       "  0.86,\n",
       "  0.87,\n",
       "  0.872,\n",
       "  0.862,\n",
       "  0.868,\n",
       "  0.86,\n",
       "  0.862,\n",
       "  0.864,\n",
       "  0.868,\n",
       "  0.866,\n",
       "  0.858,\n",
       "  0.868,\n",
       "  0.862,\n",
       "  0.852,\n",
       "  0.87,\n",
       "  0.87,\n",
       "  0.874,\n",
       "  0.872,\n",
       "  0.86,\n",
       "  0.86,\n",
       "  0.862,\n",
       "  0.86,\n",
       "  0.87,\n",
       "  0.866,\n",
       "  0.876,\n",
       "  0.858,\n",
       "  0.87,\n",
       "  0.872,\n",
       "  0.866,\n",
       "  0.87,\n",
       "  0.878,\n",
       "  0.858,\n",
       "  0.866,\n",
       "  0.86,\n",
       "  0.858,\n",
       "  0.87,\n",
       "  0.87,\n",
       "  0.868,\n",
       "  0.86,\n",
       "  0.868,\n",
       "  0.87,\n",
       "  0.864,\n",
       "  0.864,\n",
       "  0.88,\n",
       "  0.866,\n",
       "  0.876,\n",
       "  0.88,\n",
       "  0.864,\n",
       "  0.868,\n",
       "  0.862,\n",
       "  0.864,\n",
       "  0.878,\n",
       "  0.872,\n",
       "  0.866,\n",
       "  0.87,\n",
       "  0.868,\n",
       "  0.87,\n",
       "  0.866,\n",
       "  0.87,\n",
       "  0.874,\n",
       "  0.866,\n",
       "  0.87,\n",
       "  0.87,\n",
       "  0.868,\n",
       "  0.876,\n",
       "  0.864,\n",
       "  0.878,\n",
       "  0.876,\n",
       "  0.866,\n",
       "  0.868,\n",
       "  0.87,\n",
       "  0.864,\n",
       "  0.88,\n",
       "  0.866,\n",
       "  0.874,\n",
       "  0.866,\n",
       "  0.87,\n",
       "  0.868,\n",
       "  0.87,\n",
       "  0.868,\n",
       "  0.862,\n",
       "  0.86],\n",
       " 'val_auc': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.001495992,\n",
       "  0.0056480113,\n",
       "  0.0013959925,\n",
       "  0.003000006,\n",
       "  0.0074080043,\n",
       "  0.004424009,\n",
       "  0.008578007,\n",
       "  0.0030199948,\n",
       "  0.007732004,\n",
       "  0.009851999,\n",
       "  0.012440002,\n",
       "  0.029221993,\n",
       "  0.023218,\n",
       "  0.034025997,\n",
       "  0.040745996,\n",
       "  0.030447999,\n",
       "  0.03550599,\n",
       "  0.042276006,\n",
       "  0.027932009,\n",
       "  0.026956016,\n",
       "  0.028290002,\n",
       "  0.024776,\n",
       "  0.035527997,\n",
       "  0.045531996,\n",
       "  0.037986003,\n",
       "  0.040586002,\n",
       "  0.04453399,\n",
       "  0.047810003,\n",
       "  0.037991993,\n",
       "  0.038202,\n",
       "  0.038317997,\n",
       "  0.049090005,\n",
       "  0.031765997,\n",
       "  0.042772006,\n",
       "  0.022015994,\n",
       "  0.053239997,\n",
       "  0.030343998,\n",
       "  0.029156005,\n",
       "  0.044525996,\n",
       "  0.052353997,\n",
       "  0.038564004,\n",
       "  0.052883994,\n",
       "  0.043268006,\n",
       "  0.030089997,\n",
       "  0.023663998,\n",
       "  0.041786,\n",
       "  0.057224005,\n",
       "  0.045144,\n",
       "  0.057843998,\n",
       "  0.046479993,\n",
       "  0.044189997,\n",
       "  0.047548,\n",
       "  0.049200006,\n",
       "  0.041275997,\n",
       "  0.046808004,\n",
       "  0.041883998,\n",
       "  0.04518,\n",
       "  0.048606005,\n",
       "  0.067192,\n",
       "  0.03946,\n",
       "  0.033685997,\n",
       "  0.050817996,\n",
       "  0.034482002,\n",
       "  0.054215997,\n",
       "  0.048496,\n",
       "  0.04453199,\n",
       "  0.048406005,\n",
       "  0.047508,\n",
       "  0.041649997,\n",
       "  0.050498,\n",
       "  0.047044,\n",
       "  0.053081993,\n",
       "  0.049498003,\n",
       "  0.059702,\n",
       "  0.056789998,\n",
       "  0.043530002,\n",
       "  0.040215995,\n",
       "  0.015351999,\n",
       "  0.040147994,\n",
       "  0.030548004,\n",
       "  0.042826,\n",
       "  0.041719995,\n",
       "  0.049494,\n",
       "  0.042228006,\n",
       "  0.032080002,\n",
       "  0.03876,\n",
       "  0.027009998,\n",
       "  0.04654,\n",
       "  0.042248003,\n",
       "  0.04621,\n",
       "  0.032108,\n",
       "  0.044069998,\n",
       "  0.048233997,\n",
       "  0.04858,\n",
       "  0.061691992,\n",
       "  0.035148002,\n",
       "  0.02831001,\n",
       "  0.055198,\n",
       "  0.048405997,\n",
       "  0.044735998,\n",
       "  0.050044,\n",
       "  0.051464006,\n",
       "  0.056006007,\n",
       "  0.038162004,\n",
       "  0.048740003,\n",
       "  0.032874007,\n",
       "  0.03542401,\n",
       "  0.048182003,\n",
       "  0.031996,\n",
       "  0.050230004,\n",
       "  0.036319997,\n",
       "  0.045116004,\n",
       "  0.041387994,\n",
       "  0.047742005,\n",
       "  0.035336003,\n",
       "  0.039723996,\n",
       "  0.044552006,\n",
       "  0.039029997,\n",
       "  0.032098003,\n",
       "  0.055952,\n",
       "  0.040277995,\n",
       "  0.028626008,\n",
       "  0.051252,\n",
       "  0.055146,\n",
       "  0.053288005,\n",
       "  0.05127,\n",
       "  0.063524,\n",
       "  0.057531998,\n",
       "  0.038591996,\n",
       "  0.040017992,\n",
       "  0.048382,\n",
       "  0.04581201,\n",
       "  0.043369997,\n",
       "  0.04063,\n",
       "  0.041555997,\n",
       "  0.03206,\n",
       "  0.043305997,\n",
       "  0.039792,\n",
       "  0.040211998,\n",
       "  0.038478006,\n",
       "  0.031751998,\n",
       "  0.045516007,\n",
       "  0.053994,\n",
       "  0.047163993,\n",
       "  0.035078,\n",
       "  0.059622005,\n",
       "  0.057517998,\n",
       "  0.031706,\n",
       "  0.056126002,\n",
       "  0.040271994,\n",
       "  0.038489997,\n",
       "  0.055317998,\n",
       "  0.022062,\n",
       "  0.035233997,\n",
       "  0.049509995,\n",
       "  0.057108004,\n",
       "  0.025462007,\n",
       "  0.03233,\n",
       "  0.056204006,\n",
       "  0.040344,\n",
       "  0.045941997,\n",
       "  0.040192004,\n",
       "  0.043341998,\n",
       "  0.038733996,\n",
       "  0.018828006,\n",
       "  0.052476004,\n",
       "  0.050526,\n",
       "  0.052388005,\n",
       "  0.053639993,\n",
       "  0.053305995,\n",
       "  0.046278,\n",
       "  0.048268005,\n",
       "  0.041741997,\n",
       "  0.033588,\n",
       "  0.046582002,\n",
       "  0.03865,\n",
       "  0.042285997,\n",
       "  0.030235998,\n",
       "  0.048598,\n",
       "  0.047254007,\n",
       "  0.039898,\n",
       "  0.041917995,\n",
       "  0.055052,\n",
       "  0.050288,\n",
       "  0.048992,\n",
       "  0.042354003,\n",
       "  0.044889994,\n",
       "  0.045164004,\n",
       "  0.036748,\n",
       "  0.035472002,\n",
       "  0.032091998,\n",
       "  0.048932,\n",
       "  0.046913993,\n",
       "  0.042165995,\n",
       "  0.058407996,\n",
       "  0.043851998]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1in",
   "language": "python",
   "name": "tf1in"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
